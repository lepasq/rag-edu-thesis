{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FqeieOC5vUB"
      },
      "source": [
        "# Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIvSXj365r2n"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "Retrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the `chunk_size`. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex `Response Evaluation` comes handy. In this blogpost, we'll guide you through the steps to determine the best `chunk size` using LlamaIndex’s `Response Evaluation` module. If you're unfamiliar with the `Response` Evaluation module, we recommend reviewing its [documentation](https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html) before proceeding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpbtWrEa53Ct"
      },
      "source": [
        "## **Why Chunk Size Matters**\n",
        "\n",
        "Choosing the right `chunk_size` is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n",
        "\n",
        "1. **Relevance and Granularity**: A small `chunk_size`, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the `similarity_top_k` setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
        "2. **Response Generation Time**: As the `chunk_size` increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
        "\n",
        "In essence, determining the optimal `chunk_size` is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use-case and dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR8jlf3358_z"
      },
      "source": [
        "## **Setup**\n",
        "\n",
        "Before embarking on the experiment, we need to ensure all requisite modules are imported:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ItNWVKRRD67j"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: llama-index in /home/addo/.local/lib/python3.11/site-packages (0.10.7)\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /home/addo/.local/lib/python3.11/site-packages (0.1.3)\n",
            "Requirement already satisfied: spacy in /home/addo/.local/lib/python3.11/site-packages (3.7.4)\n",
            "Requirement already satisfied: llama-index-embeddings-huggingface in /home/addo/.local/lib/python3.11/site-packages (0.1.1)\n",
            "Requirement already satisfied: qdrant-client in /home/addo/.local/lib/python3.11/site-packages (1.8.0)\n",
            "Requirement already satisfied: llama-index-vector-stores-qdrant in /home/addo/.local/lib/python3.11/site-packages (0.1.4)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.10.7)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.2)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.2)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.26.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /home/addo/.local/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.20.2)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-embeddings-huggingface) (2.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-embeddings-huggingface) (4.37.2)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /home/addo/.local/lib/python3.11/site-packages (from qdrant-client) (1.62.1)\n",
            "Requirement already satisfied: grpcio-tools>=1.41.0 in /home/addo/.local/lib/python3.11/site-packages (from qdrant-client) (1.62.1)\n",
            "Requirement already satisfied: httpx>=0.14.0 in /home/addo/.local/lib/python3.11/site-packages (from httpx[http2]>=0.14.0->qdrant-client) (0.26.0)\n",
            "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /home/addo/.local/lib/python3.11/site-packages (from qdrant-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /home/addo/.local/lib/python3.11/site-packages (from qdrant-client) (2.2.1)\n",
            "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /usr/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (4.24.3)\n",
            "Requirement already satisfied: anyio in /home/addo/.local/lib/python3.11/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (4.2.0)\n",
            "Requirement already satisfied: certifi in /usr/lib/python3.11/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/addo/.local/lib/python3.11/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/lib/python3.11/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (3.4)\n",
            "Requirement already satisfied: sniffio in /home/addo/.local/lib/python3.11/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/addo/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (0.14.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /home/addo/.local/lib/python3.11/site-packages (from httpx[http2]>=0.14.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: filelock in /usr/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/addo/.local/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2023.12.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/addo/.local/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.8.0)\n",
            "Requirement already satisfied: aiohttp in /home/addo/.local/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.9.3)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/addo/.local/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.0.27)\n",
            "Requirement already satisfied: dataclasses-json in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.8)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.1.13)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.8.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.1)\n",
            "Requirement already satisfied: pandas in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (9.0.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.5.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (0.0.2)\n",
            "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (1.23.24)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/addo/.local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/addo/.local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: sympy in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (1.12)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /home/addo/.local/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/addo/.local/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (12.3.101)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/addo/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/addo/.local/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface) (0.4.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/addo/.local/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: platformdirs in /usr/lib/python3.11/site-packages (from setuptools->spacy) (3.11.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/lib/python3.11/site-packages (from setuptools->spacy) (3.11.1)\n",
            "Requirement already satisfied: more-itertools in /usr/lib/python3.11/site-packages (from setuptools->spacy) (10.1.0)\n",
            "Requirement already satisfied: ordered-set in /usr/lib/python3.11/site-packages (from setuptools->spacy) (4.1.0)\n",
            "Requirement already satisfied: tomli in /usr/lib/python3.11/site-packages (from setuptools->spacy) (2.0.1)\n",
            "Requirement already satisfied: validate-pyproject in /usr/lib/python3.11/site-packages (from setuptools->spacy) (0.13.post1.dev0+gb752273.d20230520)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /home/addo/.local/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.0 in /home/addo/.local/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (6.0.1)\n",
            "Requirement already satisfied: hpack<5,>=4.0 in /home/addo/.local/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (4.0.0)\n",
            "Requirement already satisfied: joblib in /home/addo/.local/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.8.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.22 in /home/addo/.local/lib/python3.11/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (1.23.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/addo/.local/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/addo/.local/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/addo/.local/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.20.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (3.9.0)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (4.3.0)\n",
            "Requirement already satisfied: autocommand in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (2.2.2)\n",
            "Requirement already satisfied: inflect in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/addo/.local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/addo/.local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/addo/.local/lib/python3.11/site-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Requirement already satisfied: fastjsonschema<=3,>=2.16.2 in /usr/lib/python3.11/site-packages (from validate-pyproject->setuptools->spacy) (2.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-embeddings-openai spacy llama-index-embeddings-huggingface qdrant-client llama-index-vector-stores-qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "y9SVm76h58de"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.core.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator,\n",
        "    CorrectnessEvaluator\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "import openai\n",
        "import time\n",
        "openai.api_key = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO21UssT6L8N"
      },
      "source": [
        "## **Load Data**\n",
        "\n",
        "Let’s load our document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "x6QdEBd-17OC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "319\n"
          ]
        }
      ],
      "source": [
        "# Load Data\n",
        "document_base_path = \"../data/web-software-development/\"\n",
        "\n",
        "def get_filepath_substring(file_path):\n",
        "    if document_base_path in file_path:\n",
        "        return file_path.split(document_base_path)[1]\n",
        "    return file_path\n",
        "\n",
        "# documents_path = f\"{document_base_path}21-working-with-databases/\" # single chapter\n",
        "documents_path = f\"{document_base_path}\" # full course\n",
        "\n",
        "reader = SimpleDirectoryReader(documents_path, recursive=True)\n",
        "\n",
        "documents = reader.load_data()\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnpPtiz56TYA"
      },
      "source": [
        "## **Question Generation**\n",
        "\n",
        "To select the right `chunk_size`, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various `chunk_sizes`. The `DatasetGenerator` will help us generate questions from the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "26BgDF3L6Z0r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of documents:  319\n",
            "=== EVAL QUESTIONS AND ANSWERS ===\n",
            "[('Why use NoSQL instead of SQL?', 'They are designed to scale horizontally.'), ('What is the difference between authentication and authorization?', 'The term authentication refers to identifying a user. The term authorization refers to the process of verifying that the user has the rights to perform the actions that the user is trying to perform.'), ('What is a UUID?', 'UUIDs are a common way of identifying resources. They are 128-bit numbers that are designed for being unique without central coordination (i.e. a service that would keep track of which identifier to assign next).')]\n"
          ]
        }
      ],
      "source": [
        "# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\n",
        "# eval_documents = documents[:20]\n",
        "eval_documents = documents\n",
        "print(\"Amount of documents: \", len(eval_documents))\n",
        "\n",
        "# data_generator = DatasetGenerator.from_documents(documents)\n",
        "# eval_questions = data_generator.generate_questions_from_nodes(num = 40)\n",
        "\n",
        "# generated from above, hardcoded to save costs\n",
        "\n",
        "\n",
        "# TODO not representative of student queries\n",
        "# too generic / not comprehension\n",
        "# get data from chatbot\n",
        "\n",
        "\n",
        "gen_eval_questions = ['what is the importance of using a database in web applications?',\n",
        "                      'What database management system will be used in this course?',\n",
        "                      'What are the learning objectives related to working with databases?',\n",
        "                      'Where can you find a tutorial for SQL basics if you need a refresher?',\n",
        "                      'How can you start using PostgreSQL according to the document?',\n",
        "                      'What is the recommended approach for taking PostgreSQL into use for development?',\n",
        "                      #   'What is the purpose of the Walking skeleton in relation to PostgreSQL?',\n",
        "                      'What are some options for running PostgreSQL locally?',\n",
        "                      'Name two hosted services that provide PostgreSQL as a service.',\n",
        "                      #   'Why does the document strongly recommend using the first option for development when starting to use PostgreSQL?',\n",
        "                      'What are two options for starting to use PostgreSQL as mentioned in the document?',\n",
        "                      'What are some examples of hosted services that provide PostgreSQL databases?',\n",
        "                      #   'Why does the document strongly recommend using the first option for development?',\n",
        "                      'How can you get started with ElephantSQL according to the document?',\n",
        "                      \"What attributes are included in the table created in the document's example using SQL?\",\n",
        "                      'How can you add names to the table in ElephantSQL according to the document?',\n",
        "                      \"What SQL query can you use to select all rows from the 'names' table in ElephantSQL?\",\n",
        "                      \"What library is used in the document's example to access the database programatically?\",\n",
        "                      #   'What information is grayed out in the image of the ElephantSQL details page?',\n",
        "                      \"What is the purpose of the 'id' attribute in the table created in the document's example?\",\n",
        "                      #   'What library is used in the first example to access a PostgreSQL database in the provided code snippet?',\n",
        "                      #   'How can you specify the database credentials when using Postgres.js in the provided code snippet?',\n",
        "                      #   'What is the purpose of the `max: 2` parameter in the Postgres.js example?',\n",
        "                      #   'In the second example, what library is used to access a PostgreSQL database?',\n",
        "                      'What is the recommended alternative to Deno Postgres mentioned in the document?',\n",
        "                      #   'How can you establish a connection to a PostgreSQL database using Deno Postgres in the provided code snippet?',\n",
        "                      #   'What query is executed in the Deno Postgres example to retrieve data from the database?',\n",
        "                      'What is the significance of having a database client when working with databases?',\n",
        "                      'What is the default database client mentioned in the document for accessing a PostgreSQL database?',\n",
        "                      'Where can you find a list of PostgreSQL clients for different operating systems according to the document?',\n",
        "                      'What database driver is used when working with Deno and PostgreSQL in the provided document?',\n",
        "                      'How can you create a database client using the Postgres.js driver?',\n",
        "                      #   'In the example code provided, what SQL query is being executed to retrieve data from the database?',\n",
        "                      'How does Postgres.js ensure safe query generation when constructing SQL queries?',\n",
        "                      'What is the purpose of the `sql` function in the Postgres.js driver?',\n",
        "                      #   'In the example code, how is the result data iterated over to print only the name property?',\n",
        "                      #   'What SQL statement is used to insert data into a database in the provided document?',\n",
        "                      #   'After inserting a new name into the database, how many names are present in the database according to the output?',\n",
        "                      'What flag is required to be used with Deno when working with the Postgres.js driver?',\n",
        "                      'How can you access the Postgres.js documentation for further details on tagged template literals?']\n",
        "\n",
        "\n",
        "eval_qa = [\n",
        "    ('Why use NoSQL instead of SQL?',\n",
        "     'They are designed to scale horizontally.'),\n",
        "\n",
        "    ('What is the difference between authentication and authorization?',\n",
        "     'The term authentication refers to identifying a user. The term authorization refers to the process of verifying that the user has the rights to perform the actions that the user is trying to perform.'),\n",
        "\n",
        "    ('What is a UUID?',\n",
        "     'UUIDs are a common way of identifying resources. They are 128-bit numbers that are designed for being unique without central coordination (i.e. a service that would keep track of which identifier to assign next).'),\n",
        "]\n",
        "\n",
        "\n",
        "def get_page_number(path):\n",
        "    substr = get_filepath_substring(path)\n",
        "    chapter, section = substr.split(\"/\")\n",
        "    chapter_no = chapter.split(\"-\")[0]\n",
        "    section_no = section.split(\"-\")[0]\n",
        "    return chapter_no + section_no\n",
        "\n",
        "print(\"=== EVAL QUESTIONS AND ANSWERS ===\")\n",
        "print(eval_qa)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WwA-0N6dMO"
      },
      "source": [
        "## Setting Up Evaluators\n",
        "\n",
        "We are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, `FaithfulnessEvaluator` and `RelevancyEvaluator`, are initialised with the `service_context` .\n",
        "\n",
        "1. **Faithfulness Evaluator** - It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.\n",
        "2. **Relevancy Evaluator** - It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "G2LoMRtr6fnG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# We will use for evaluating the responses\n",
        "\n",
        "\n",
        "gpt35 = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
        "# gpt4 = OpenAI(model=\"gpt-4\", temperature=0.2)\n",
        "\n",
        "roberta = HuggingFaceEmbedding(model_name=\"deepset/roberta-base-squad2\")\n",
        "\n",
        "\n",
        "def get_openai_text_embedding_3_large(chunk_size):\n",
        "    return OpenAIEmbedding(model=\"text-embedding-3-large\", chunk_size=chunk_size,)\n",
        "\n",
        "def get_openai_text_embedding_3_small(chunk_size):\n",
        "    return OpenAIEmbedding(model=\"text-embedding-3-small\", chunk_size=chunk_size,)\n",
        "\n",
        "def get_openai_text_embedding_ada_002(chunk_size):\n",
        "    return OpenAIEmbedding(model=\"text-embedding-ada-002\", chunk_size=chunk_size,)\n",
        "\n",
        "def get_roberta_text_embedding():\n",
        "    return roberta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_617772/3108636645.py:4: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context_gpt_35 = ServiceContext.from_defaults(llm=llm_evaluate)\n"
          ]
        }
      ],
      "source": [
        "llm_evaluate = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define service context for llm evaluation\n",
        "service_context_gpt_35 = ServiceContext.from_defaults(llm=llm_evaluate)\n",
        "\n",
        "# Define Faithfulness and Relevancy Evaluators\n",
        "faithfulness_gpt = FaithfulnessEvaluator(\n",
        "    service_context=service_context_gpt_35)\n",
        "relevancy_gpt = RelevancyEvaluator(service_context=service_context_gpt_35)\n",
        "correctness_gpt = CorrectnessEvaluator(service_context=service_context_gpt_35)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (DO NOT RUN) Debugging local embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from llama_index.core import Settings\n",
        "\n",
        "# from llama_index.core.schema import IndexNode\n",
        "# from llama_index.core import (\n",
        "#     load_index_from_storage,\n",
        "#     StorageContext,\n",
        "#     VectorStoreIndex,\n",
        "# )\n",
        "# from llama_index.core.node_parser import SentenceSplitter\n",
        "# from llama_index.core import SummaryIndex\n",
        "# from llama_index.core.retrievers import RecursiveRetriever\n",
        "# import os\n",
        "# # from tqdm.notebook import tqdm\n",
        "# import pickle\n",
        "\n",
        "\n",
        "# def build_index_local(docs, chunk_size, out_path: str):\n",
        "#     print(\"Chunk size: \", chunk_size)\n",
        "\n",
        "#     Settings.embed_model = embed_model\n",
        "\n",
        "#     nodes = []\n",
        "\n",
        "#     splitter = SentenceSplitter(\n",
        "#         chunk_size=chunk_size, chunk_overlap=chunk_size/4)\n",
        "#     for idx, doc in enumerate(docs):\n",
        "#         print('Splitting: ' + str(idx))\n",
        "\n",
        "#         cur_nodes = splitter.get_nodes_from_documents([doc])\n",
        "#         for cur_node in cur_nodes:\n",
        "#             # ID will be base + parent\n",
        "#             file_path = doc.metadata[\"file_path\"].split(document_base_path)[1]\n",
        "#             new_node = IndexNode(\n",
        "#                 text=cur_node.text or \"None\",\n",
        "#                 index_id=str(file_path),\n",
        "#                 metadata=doc.metadata,\n",
        "#                 # obj=doc\n",
        "#             )\n",
        "#             nodes.append(new_node)\n",
        "        \n",
        "\n",
        "#         # Debugging\n",
        "#         print(len(cur_nodes), len(str(doc)), len(str(cur_nodes[0])))\n",
        "#         for xyz in cur_nodes:\n",
        "#             print(xyz)\n",
        "#             print(\"-\")\n",
        "#         print()\n",
        "#         print(\"----DOC-----\")\n",
        "#         print(doc)\n",
        "\n",
        "#         print()\n",
        "#         print()\n",
        "\n",
        "#     print(\"num nodes: \" + str(len(nodes)))\n",
        "\n",
        "#     service_context = ServiceContext.from_defaults(\n",
        "#         llm=llm_evaluate, embed_model=embed_model)\n",
        "\n",
        "#     # save index to disk\n",
        "#     if not os.path.exists(out_path):\n",
        "#         index = VectorStoreIndex(nodes, service_context=service_context)\n",
        "#         index.set_index_id(\"simple_index\")\n",
        "#         index.storage_context.persist(f\"./{out_path}\")\n",
        "#     else:\n",
        "#         # rebuild storage context\n",
        "#         storage_context = StorageContext.from_defaults(\n",
        "#             persist_dir=f\"./{out_path}\"\n",
        "#         )\n",
        "#         # load index\n",
        "#         index = load_index_from_storage(\n",
        "#             storage_context, index_id=\"simple_index\", service_context=service_context\n",
        "#             # storage_context, index_id=\"simple_index\", embed_model=embed_model\n",
        "#         )\n",
        "\n",
        "#     return index\n",
        "\n",
        "\n",
        "# # build_index_local(eval_documents, 1024, \"Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# vs. Storing embeddings (vector database)\n",
        "`docker-compose -f docker-compose.weaviate-persistent.yml up`\n",
        "\n",
        "OR  \n",
        "\n",
        "`docker-compose -f docker-compose.qdrant.yml up`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text-embedding-3-large with 128 chunk size -> W128te3l\n",
        "# W because has to start with capital letter\n",
        "def get_full_course_index(chunk_size, model_name):\n",
        "    return f\"W{chunk_size}{''.join([x[0] for x in model_name.split('-')])}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from llama_index.core.schema import IndexNode\n",
        "from llama_index.core import (\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.retrievers import RecursiveRetriever\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "import qdrant_client\n",
        "from qdrant_client.http import models\n",
        "\n",
        "client = qdrant_client.QdrantClient(\n",
        "    \"http://localhost:6333\",\n",
        "    api_key=None, \n",
        ")\n",
        "\n",
        "\n",
        "def build_index(docs, chunk_size, embed_model, llm):\n",
        "\n",
        "    print(\"existing: \", client.get_collections())\n",
        "    print(\"Chunk size: \", chunk_size)\n",
        "\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    service_context = ServiceContext.from_defaults(\n",
        "        llm=llm, embed_model=embed_model)\n",
        "\n",
        "    index_name = get_full_course_index(chunk_size, embed_model.model_name)\n",
        "\n",
        "    try:\n",
        "        client.get_collection(index_name)\n",
        "        vector_store = QdrantVectorStore(client=client, collection_name=index_name, enable_hybrid=True, batch_size=15)\n",
        "        index = VectorStoreIndex.from_vector_store(vector_store, service_context=service_context)\n",
        "        print(f\"Schema {index_name} exists already, load cached from database\")\n",
        "    except:\n",
        "        print(f\"Schema {index_name} does not exist, rebuilding and then storing in db\")\n",
        "        nodes = []\n",
        "        splitter = SentenceSplitter(\n",
        "            chunk_size=chunk_size, chunk_overlap=chunk_size/4)\n",
        "        for idx, doc in enumerate(docs):\n",
        "            print('Splitting: ' + str(idx))\n",
        "\n",
        "            cur_nodes = splitter.get_nodes_from_documents([doc])\n",
        "            for cur_node in cur_nodes:\n",
        "                # ID will be base + parent\n",
        "                file_path = get_filepath_substring(doc.metadata[\"file_path\"])\n",
        "                new_node = IndexNode(\n",
        "                    text=cur_node.text or \"None\",\n",
        "                    index_id=str(file_path),\n",
        "                    metadata=doc.metadata,\n",
        "                    # obj=doc\n",
        "                )\n",
        "                nodes.append(new_node)\n",
        "\n",
        "        print(\"num nodes: \" + str(len(nodes)))\n",
        "\n",
        "        vector_store = QdrantVectorStore(client=client, collection_name=index_name, enable_hybrid=True, batch_size=15)\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "        index = VectorStoreIndex(nodes, storage_context = storage_context, service_context=service_context)\n",
        "\n",
        "\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <chunk_size>, <k>, <use_hybrid>, <model>, <use_rag>\n",
        "\n",
        "# config_lst = [(128, 10, True, gpt35, True), (128, 8, True, gpt35, True), (256, 6, True, gpt35, True),\n",
        "#  (512, 4, True, gpt35, True), (1024, 2, True, gpt35, True), (2048, 1, True, gpt35, True)]\n",
        "\n",
        "openai_3_128_large = get_openai_text_embedding_3_large(128)\n",
        "# openai_3_128_small = get_openai_text_embedding_3_small(128)\n",
        "openai_ada_002_128 = get_openai_text_embedding_ada_002(128)\n",
        "roberta = get_roberta_text_embedding()\n",
        "\n",
        "config_lst = [\n",
        "              (128, 8, True, gpt35, True, openai_3_128_large), \n",
        "              # (128, 8, True, gpt35, True, openai_ada_002_128), \n",
        "              # (128, 8, True, gpt35, True, roberta), \n",
        "              (128, 8, False, gpt35, True, openai_3_128_large), \n",
        "            #   (128, 8, False, gpt35, True, openai_ada_002_128), \n",
        "            #   (128, 8, False, gpt35, True, roberta), \n",
        "              (128, 8, True, gpt35, False, openai_3_128_large),\n",
        "              ]\n",
        "\n",
        "n_runs = len(config_lst)\n",
        "\n",
        "answers = [[] for _ in range(n_runs)]\n",
        "sources = [[] for _ in range(n_runs)]\n",
        "source_docs = [[] for _ in range(n_runs)]\n",
        "\n",
        "time_scores = [[] for _ in range(n_runs)]\n",
        "faithfulness_scores = [[] for _ in range(n_runs)]\n",
        "relevancy_scores = [[] for _ in range(n_runs)]\n",
        "correctness_scores = [[] for _ in range(n_runs)]\n",
        "\n",
        "faithfulness_false = [[] for _ in range(n_runs)]\n",
        "relevancy_false = [[] for _ in range(n_runs)]\n",
        "correctness_false = [[] for _ in range(n_runs)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_617772/573311361.py:35: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "existing:  collections=[CollectionDescription(name='W128te3l')]\n",
            "Chunk size:  128\n",
            "Schema W128te3l exists already, load cached from database\n",
            "== BUILDING QUERY ENGINE ==\n",
            "== BUILDING CHAT ENGINE ==\n",
            "existing:  collections=[CollectionDescription(name='W128te3l')]\n",
            "Chunk size:  128\n",
            "Schema W128te3l exists already, load cached from database\n",
            "== BUILDING QUERY ENGINE ==\n",
            "== BUILDING CHAT ENGINE ==\n",
            "existing:  collections=[CollectionDescription(name='W128te3l')]\n",
            "Chunk size:  128\n",
            "Schema W128te3l exists already, load cached from database\n",
            "== BUILDING QUERY ENGINE ==\n",
            "== BUILDING CHAT ENGINE ==\n"
          ]
        }
      ],
      "source": [
        "vector_indices = []\n",
        "query_engines = []\n",
        "chat_engines = []\n",
        "\n",
        "# Helper methods\n",
        "\n",
        "def build_query_engine(vector_index, similarity_top_k, is_hybrid, embed_model):\n",
        "    print(\"== BUILDING QUERY ENGINE ==\")\n",
        "    if is_hybrid:\n",
        "        return vector_index.as_query_engine(\n",
        "             embed_model=embed_model, \n",
        "             similarity_top_k=similarity_top_k//2, \n",
        "             sparse_top_k=similarity_top_k//2, \n",
        "             vector_store_query_mode=\"hybrid\"\n",
        "\n",
        "        )\n",
        "    # -- VEC ONLY --\n",
        "    return vector_index.as_query_engine(\n",
        "        similarity_top_k=similarity_top_k, embed_model=embed_model,\n",
        "    )\n",
        "\n",
        "\n",
        "def build_chat_engine(vector_index, similarity_top_k, is_hybrid, embed_model):\n",
        "    print(\"== BUILDING CHAT ENGINE ==\")\n",
        "    if is_hybrid:\n",
        "        return vector_index.as_chat_engine(\n",
        "            similarity_top_k=similarity_top_k, embed_model=embed_model,\n",
        "            vector_store_query_mode=\"hybrid\", alpha=0.0  # BM25\n",
        "        )\n",
        "    # -- VEC ONLY --\n",
        "    return vector_index.as_chat_engine(\n",
        "        similarity_top_k=similarity_top_k, embed_model=embed_model,\n",
        "    )\n",
        "\n",
        "def build_all_indices():\n",
        "    for run_i, (chunk_size, similarity_top_k, is_hybrid, llm, is_rag, embed_model) in enumerate(config_lst):\n",
        "        vector_index = build_index(eval_documents, chunk_size, embed_model, llm) \n",
        "        vector_indices.append(vector_index)\n",
        "        query_engines.append(build_query_engine(vector_index, similarity_top_k, is_hybrid, embed_model))\n",
        "        chat_engines.append(build_chat_engine(vector_index, similarity_top_k, is_hybrid, embed_model))\n",
        "\n",
        "build_all_indices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUncIIxR6gVz"
      },
      "source": [
        "## **Response Evaluation For A Chunk Size**\n",
        "\n",
        "We evaluate each chunk_size based on 3 metrics.\n",
        "\n",
        "1. Average Response Time.\n",
        "2. Average Faithfulness.\n",
        "3. Average Relevancy.\n",
        "\n",
        "Here's a function, `evaluate_config`, that does just that which has:\n",
        "\n",
        "1. VectorIndex Creation.\n",
        "2. Building the Query Engine**.**\n",
        "3. Metrics Calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "dEC2Lr0z6p1N"
      },
      "outputs": [],
      "source": [
        "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
        "# We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n",
        "from llama_index.core.base.response.schema import Response\n",
        "\n",
        "\n",
        "def evaluate_config(chunk_size, similarity_top_k, llm, run_i, embed_model, eval_qa=eval_qa, label=\"default\", is_hybrid=True, is_rag=True, is_chat=True):\n",
        "    \"\"\"\n",
        "    Evaluate the average response time, faithfulness, and relevancy of responses for a given chunk size.\n",
        "    \"\"\"\n",
        "\n",
        "    Settings.llm = llm\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    chat_engine = chat_engines[run_i]\n",
        "    query_engine = query_engines[run_i]\n",
        "    # Iterate over each question in eval_questions to compute metrics.\n",
        "    for q, a in eval_qa:\n",
        "        print(\"--Q: \", q)\n",
        "        start_time = time.time()\n",
        "\n",
        "        if is_rag:\n",
        "            if is_chat:\n",
        "                response_vector = chat_engine.chat(q)\n",
        "                raw_file_paths = list(set(value['file_path']\n",
        "                                        for value in response_vector.sources[0].raw_output.metadata.values()))\n",
        "                source_file_paths = list(\n",
        "                    map(get_filepath_substring, raw_file_paths))\n",
        "                source_docs[run_i].append(response_vector.source_nodes)\n",
        "\n",
        "            else:\n",
        "                response_vector = query_engine.query(q)\n",
        "\n",
        "                # get unique list of file paths of source docs\n",
        "                if response_vector.metadata is not None:\n",
        "                    print(\"--sources: \")\n",
        "                    raw_file_paths = list(set(value['file_path']\n",
        "                                            for value in response_vector.metadata.values()))\n",
        "                    source_file_paths = list(\n",
        "                        map(get_filepath_substring, raw_file_paths))\n",
        "                else:\n",
        "                    source_file_paths = []\n",
        "                source_docs[run_i].append(response_vector.get_formatted_sources)\n",
        "\n",
        "\n",
        "            print(\"--A: \", str(response_vector))\n",
        "            sources[run_i].append(source_file_paths)\n",
        "            print(\"----------\")\n",
        "\n",
        "\n",
        "        else: # NO RAG\n",
        "            # this seems to be the case with RAG systems\n",
        "            q += \" Make the answer one or two sentence long.\"\n",
        "            response_vector = None\n",
        "            response = str(OpenAI().complete(q))\n",
        "            print(\"--A: \", str(response))\n",
        "            response_vector = Response(response)\n",
        "            sources[run_i].append([])\n",
        "\n",
        "        answers[run_i].append(response_vector)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        faithfulness_result = faithfulness_gpt.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing if is_rag else False\n",
        "\n",
        "        relevancy_result = relevancy_gpt.evaluate_response(\n",
        "            query=q, response=response_vector\n",
        "        ).passing if is_rag else False\n",
        "\n",
        "        if not faithfulness_result:\n",
        "            faithfulness_false[run_i].append((q, str(response_vector)))\n",
        "        if not relevancy_result:\n",
        "            relevancy_false[run_i].append((q, str(response_vector)))\n",
        "\n",
        "        try:\n",
        "            correctness_result = correctness_gpt.evaluate(\n",
        "                query=q,\n",
        "                response=str(response_vector),\n",
        "                reference=a,\n",
        "            )\n",
        "\n",
        "            correctness_scores[run_i].append(\n",
        "                (correctness_result.score, correctness_result.feedback))\n",
        "            if correctness_result.score < 2.0:\n",
        "                correctness_false[run_i].append(\n",
        "                    (q, str(response_vector), (correctness_result.score, correctness_result.feedback)))\n",
        "        except:\n",
        "            print(\"Failed to parse correctness score\")\n",
        "            correctness_scores[run_i].append(None)\n",
        "\n",
        "        time_scores[run_i].append(elapsed_time)\n",
        "        faithfulness_scores[run_i].append(faithfulness_result)\n",
        "        relevancy_scores[run_i].append(relevancy_result)\n",
        "        # print(\n",
        "        #     f\"t={elapsed_time}, f={faithfulness_result}, r={relevancy_result}, c={correctness_result.score}\\n-------\")\n",
        "\n",
        "    print(\"===========\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8DQvTP96s48"
      },
      "source": [
        "## **Testing Across Different Chunk Sizes**\n",
        "\n",
        "We'll evaluate a range of chunk sizes to identify which offers the most promising metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "jlKICwXH6Tib"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--Q:  Why use NoSQL instead of SQL?\n",
            "--A:  NoSQL databases are often preferred over SQL databases due to concerns about scalability. NoSQL databases are designed to scale horizontally and are often schemaless, allowing for more flexibility in handling large amounts of data. Additionally, NoSQL databases like MongoDB and Redis have evolved to support complex data models, offering a unified approach that combines the strengths of both SQL and NoSQL databases, known as NewSQL.\n",
            "----------\n",
            "Why use NoSQL instead of SQL? NoSQL databases are often preferred over SQL databases due to concerns about scalability. NoSQL databases are designed to scale horizontally and are often schemaless, allowing for more flexibility in handling large amounts of data. Additionally, NoSQL databases like MongoDB and Redis have evolved to support complex data models, offering a unified approach that combines the strengths of both SQL and NoSQL databases, known as NewSQL. They are designed to scale horizontally.\n",
            "--Q:  What is the difference between authentication and authorization?\n",
            "--A:  Authentication refers to the process of identifying a user, typically through means like something they know, have, or are, such as passwords or biometric data. On the other hand, authorization involves verifying that the user has the rights to perform specific actions or access particular resources within an application.\n",
            "----------\n",
            "What is the difference between authentication and authorization? Authentication refers to the process of identifying a user, typically through means like something they know, have, or are, such as passwords or biometric data. On the other hand, authorization involves verifying that the user has the rights to perform specific actions or access particular resources within an application. The term authentication refers to identifying a user. The term authorization refers to the process of verifying that the user has the rights to perform the actions that the user is trying to perform.\n",
            "Failed to parse correctness score\n",
            "--Q:  What is a UUID?\n",
            "--A:  A UUID is a 128-bit number that is designed to be unique without requiring central coordination. It stands for Universally Unique Identifier and is standardized in RFC 4122.\n",
            "----------\n",
            "What is a UUID? A UUID is a 128-bit number that is designed to be unique without requiring central coordination. It stands for Universally Unique Identifier and is standardized in RFC 4122. UUIDs are a common way of identifying resources. They are 128-bit numbers that are designed for being unique without central coordination (i.e. a service that would keep track of which identifier to assign next).\n",
            "Failed to parse correctness score\n",
            "===========\n",
            "--Q:  Why use NoSQL instead of SQL?\n",
            "--A:  NoSQL databases are often preferred over SQL databases due to their ability to scale horizontally and their schemaless nature. They were designed to address concerns about the scalability of relational databases, especially in web development scenarios. Additionally, NoSQL databases like MongoDB and Redis have evolved to support complex data models, offering a flexible approach to data storage.\n",
            "----------\n",
            "Why use NoSQL instead of SQL? NoSQL databases are often preferred over SQL databases due to their ability to scale horizontally and their schemaless nature. They were designed to address concerns about the scalability of relational databases, especially in web development scenarios. Additionally, NoSQL databases like MongoDB and Redis have evolved to support complex data models, offering a flexible approach to data storage. They are designed to scale horizontally.\n",
            "--Q:  What is the difference between authentication and authorization?\n",
            "--A:  Authentication involves verifying the identity of a user, typically through means like passwords or biometric data, to confirm they are who they claim to be. On the other hand, authorization is the process of determining what actions a user is allowed to perform and what resources they can access after their identity has been authenticated.\n",
            "----------\n",
            "What is the difference between authentication and authorization? Authentication involves verifying the identity of a user, typically through means like passwords or biometric data, to confirm they are who they claim to be. On the other hand, authorization is the process of determining what actions a user is allowed to perform and what resources they can access after their identity has been authenticated. The term authentication refers to identifying a user. The term authorization refers to the process of verifying that the user has the rights to perform the actions that the user is trying to perform.\n",
            "Failed to parse correctness score\n",
            "--Q:  What is a UUID?\n",
            "--A:  A UUID is a universally unique identifier string that is randomly generated.\n",
            "----------\n",
            "What is a UUID? A UUID is a universally unique identifier string that is randomly generated. UUIDs are a common way of identifying resources. They are 128-bit numbers that are designed for being unique without central coordination (i.e. a service that would keep track of which identifier to assign next).\n",
            "===========\n",
            "--Q:  Why use NoSQL instead of SQL?\n",
            "--A:  NoSQL databases are better suited for handling large volumes of unstructured data and provide more flexibility in schema design compared to SQL databases.\n",
            "Why use NoSQL instead of SQL? Make the answer one or two sentence long. NoSQL databases are better suited for handling large volumes of unstructured data and provide more flexibility in schema design compared to SQL databases. They are designed to scale horizontally.\n",
            "--Q:  What is the difference between authentication and authorization?\n",
            "--A:  Authentication is the process of verifying the identity of a user, while authorization is the process of determining what actions a user is allowed to perform.\n",
            "What is the difference between authentication and authorization? Make the answer one or two sentence long. Authentication is the process of verifying the identity of a user, while authorization is the process of determining what actions a user is allowed to perform. The term authentication refers to identifying a user. The term authorization refers to the process of verifying that the user has the rights to perform the actions that the user is trying to perform.\n",
            "--Q:  What is a UUID?\n",
            "--A:  A UUID (Universally Unique Identifier) is a 128-bit identifier that is used to uniquely identify information in computer systems without needing a central authority to assign them.\n",
            "What is a UUID? Make the answer one or two sentence long. A UUID (Universally Unique Identifier) is a 128-bit identifier that is used to uniquely identify information in computer systems without needing a central authority to assign them. UUIDs are a common way of identifying resources. They are 128-bit numbers that are designed for being unique without central coordination (i.e. a service that would keep track of which identifier to assign next).\n",
            "Failed to parse correctness score\n",
            "===========\n"
          ]
        }
      ],
      "source": [
        "from statistics import mean\n",
        "\n",
        "# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n",
        "for run_i, (chunk_size, similarity_top_k, is_hybrid, llm, is_rag, embed_model) in enumerate(config_lst):\n",
        "    evaluate_config(chunk_size, similarity_top_k, llm, run_i, embed_model=embed_model, eval_qa=eval_qa, is_hybrid=is_hybrid, is_rag=is_rag)\n",
        "\n",
        "evaluation_model_name = llm_evaluate.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= STATS ============\n",
            "n_questions: 3\n",
            "evaluation model: gpt-3.5-turbo\n",
            "============= MODELS ===========\n",
            "(hyb-gpt-3.5-turbo-text-embedding-3-large-128*8-rag) - avg time: 6.64s, avg faithf: 1.00, avg relev: 1.00 avg correct: 4.50\n",
            "(vec-gpt-3.5-turbo-text-embedding-3-large-128*8-rag) - avg time: 7.24s, avg faithf: 1.00, avg relev: 0.50 avg correct: 4.25\n",
            "(hyb-gpt-3.5-turbo-text-embedding-3-large-128*8) - avg time: 1.06s, avg faithf: 0.00, avg relev: 0.00 avg correct: 4.00\n",
            "============= QA ============\n",
            "========================================================\n",
            "---Q evaluated by gpt-3.5-turbo: Why use NoSQL instead of SQL? --- ref answer: They are designed to scale horizontally.\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-text-embedding-3-large-128*8-rag): NoSQL databases are often preferred over SQL databases due to concerns about scalability. NoSQL databases are designed to scale horizontally and are often schemaless, allowing for more flexibility in handling large amounts of data. Additionally, NoSQL databases like MongoDB and Redis have evolved to support complex data models, offering a unified approach that combines the strengths of both SQL and NoSQL databases, known as NewSQL.\n",
            "t=7.274688720703125s f=True r=True c=(4.5, 'The generated answer provides a detailed explanation of why NoSQL is preferred over SQL, mentioning concerns about scalability, horizontal scaling, schema flexibility, and support for complex data models. It also introduces the concept of NewSQL. The answer is relevant, correct, and goes beyond the basic explanation, earning a high score.')\n",
            "sources: ['6-data-on-server/2-key-value-stores.mdx', '19-evolution-of-web-development/3-rest-client-side-frameworks-and-containerization.mdx', '21-working-with-databases/3-user-specific-data.mdx', 'changelog.mdx']\n",
            "===\n",
            "(vec-gpt-3.5-turbo-text-embedding-3-large-128*8-rag): NoSQL databases are often preferred over SQL databases due to their ability to scale horizontally and their schemaless nature. They were designed to address concerns about the scalability of relational databases, especially in web development scenarios. Additionally, NoSQL databases like MongoDB and Redis have evolved to support more complex data models, offering a flexible approach to data storage.\n",
            "t=6.345729112625122s f=True r=False c=(4.5, 'The generated answer provides a comprehensive explanation of why NoSQL is preferred over SQL, mentioning the ability to scale horizontally, schemaless nature, and addressing scalability concerns in web development scenarios. It also gives examples of NoSQL databases like MongoDB and Redis. The answer is relevant and correct, but could be slightly more concise for a higher score.')\n",
            "sources: ['6-data-on-server/2-key-value-stores.mdx', '19-evolution-of-web-development/3-rest-client-side-frameworks-and-containerization.mdx', '21-working-with-databases/2-databases-and-web-applications.mdx']\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-text-embedding-3-large-128*8): NoSQL databases are better suited for handling large volumes of unstructured data and provide more flexibility in schema design compared to SQL databases.\n",
            "t=1.0982322692871094s f=False r=False c=(3.5, 'The generated answer provides relevant information about the advantages of NoSQL over SQL, but it is slightly longer than the requested one or two sentence limit.')\n",
            "sources: []\n",
            "\n",
            "\n",
            "\n",
            "========================================================\n",
            "---Q evaluated by gpt-3.5-turbo: What is the difference between authentication and authorization? --- ref answer: The term authentication refers to identifying a user. The term authorization refers to the process of verifying that the user has the rights to perform the actions that the user is trying to perform.\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-text-embedding-3-large-128*8-rag): Authentication refers to the process of identifying a user, typically through something they know, have, or are, like passwords or biometric data. On the other hand, authorization is the process of verifying that the user has the rights to perform specific actions or access particular resources within an application.\n",
            "t=5.148356676101685s f=True r=True c=None\n",
            "sources: ['13-authentication-and-authorization/1-authentication.mdx', '13-authentication-and-authorization/2-authorization.mdx']\n",
            "===\n",
            "(vec-gpt-3.5-turbo-text-embedding-3-large-128*8-rag): Authentication involves verifying the identity of a user, typically through means like passwords or biometric data, to confirm that the user is who they claim to be. On the other hand, authorization is the process of determining what actions a user is allowed to perform and what resources they can access after their identity has been authenticated.\n",
            "t=7.152766227722168s f=True r=False c=(4.5, 'Reasoning:\\nThe generated answer provides a clear and accurate explanation of the difference between authentication and authorization. It correctly defines both terms and explains their roles in the user access control process. The answer is well-structured and covers all the key points mentioned in the reference answer. However, it could be slightly improved by being more concise without losing any important information.')\n",
            "sources: ['13-authentication-and-authorization/1-authentication.mdx', '13-authentication-and-authorization/index.mdx', '13-authentication-and-authorization/2-authorization.mdx']\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-text-embedding-3-large-128*8): Authentication is the process of verifying the identity of a user, while authorization is the process of determining what actions a user is allowed to perform.\n",
            "t=1.0179810523986816s f=False r=False c=(4.5, 'The generated answer provides a clear and concise explanation of the difference between authentication and authorization, matching the user query and reference answer closely.')\n",
            "sources: []\n",
            "\n",
            "\n",
            "\n",
            "========================================================\n",
            "---Q evaluated by gpt-3.5-turbo: What is a UUID? --- ref answer: UUIDs are a common way of identifying resources. They are 128-bit numbers that are designed for being unique without central coordination (i.e. a service that would keep track of which identifier to assign next).\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-text-embedding-3-large-128*8-rag): A UUID is a 128-bit number that is designed to be unique without requiring central coordination. It stands for Universally Unique Identifier and is standardized in RFC 4122.\n",
            "t=4.123837471008301s f=True r=True c=(4.5, 'The generated answer provides a clear and accurate explanation of what a UUID is, mentioning its purpose, structure, and standardization. It also includes additional information about the abbreviation \"UUID\" and the standard RFC 4122, which enhances the completeness of the answer.')\n",
            "sources: ['12-cookies-and-sessions/5-sessions.mdx', '10-the-crud/3-create.mdx', '22-application-programming-interfaces/5-api-comprehension.mdx', '21-working-with-databases/2-databases-and-web-applications.mdx', '2-internet-and-http/1-introduction-to-the-internet.mdx', '14-user-management/3-registration.mdx']\n",
            "===\n",
            "(vec-gpt-3.5-turbo-text-embedding-3-large-128*8-rag): A UUID is a universally unique identifier string that is randomly generated. It is used to uniquely identify entities or resources in a system.\n",
            "t=3.8349597454071045s f=True r=True c=None\n",
            "sources: ['16-user-specific-data/1-overview.mdx', '12-cookies-and-sessions/5-sessions.mdx', '22-application-programming-interfaces/5-api-comprehension.mdx', '99-requesting-credits/2-requesting-grading.mdx', '2-internet-and-http/1-introduction-to-the-internet.mdx']\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-text-embedding-3-large-128*8): A UUID (Universally Unique Identifier) is a 128-bit number used to uniquely identify information in computer systems, ensuring no two items have the same identifier.\n",
            "t=1.1662089824676514s f=False r=False c=None\n",
            "sources: []\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"============= STATS ============\")\n",
        "print(f\"n_questions: {len(eval_qa)}\")\n",
        "print(f\"evaluation model: {evaluation_model_name}\")\n",
        "print(\"============= MODELS ===========\")\n",
        "\n",
        "for config_i, (chunk_size, similarity_top_k, is_hybrid, llm, is_rag, embed_model) in enumerate(config_lst):\n",
        "    time_avg = mean(time_scores[config_i])\n",
        "    faithfulness_avg = mean(faithfulness_scores[config_i])\n",
        "    relevancy_avg = mean(relevancy_scores[config_i])\n",
        "    correctness_without_none = [\n",
        "        x for x in correctness_scores[config_i] if x is not None]\n",
        "    correctness_avg = sum(\n",
        "        elt[0] for elt in correctness_without_none)/len(correctness_without_none) if len(correctness_without_none) > 0 else -1\n",
        "    print(\n",
        "        f\"({'hyb' if is_hybrid else 'vec'}-{llm.model}-{embed_model.model_name}-{chunk_size}*{similarity_top_k}{'-rag' if is_rag else ''}) - avg time: {time_avg:.2f}s, avg faithf: {faithfulness_avg:.2f}, avg relev: {relevancy_avg:.2f} avg correct: {correctness_avg:.2f}\")\n",
        "\n",
        "\n",
        "print(\"============= QA ============\")\n",
        "\n",
        "for i, (q, a) in enumerate(eval_qa):\n",
        "    print(\"========================================================\")\n",
        "    print(\n",
        "        f\"---Q evaluated by {evaluation_model_name}: {q} --- ref answer: {a}\")\n",
        "    for config_i, (chunk_size, similarity_top_k, is_hybrid, llm, is_rag, embed_model) in enumerate(config_lst):\n",
        "        print(\"===\")\n",
        "        print(\n",
        "            f\"({'hyb' if is_hybrid else 'vec'}-{llm.model}-{embed_model.model_name}-{chunk_size}*{similarity_top_k}{'-rag' if is_rag else ''}): {answers[config_i][i]}\")\n",
        "        print(\n",
        "            f\"t={time_scores[config_i][i]}s f={faithfulness_scores[config_i][i]} r={relevancy_scores[config_i][i]} c={correctness_scores[config_i][i]}\")\n",
        "        print(\n",
        "            f\"sources: {sources[config_i][i]}\")\n",
        "    print()\n",
        "    print()\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
