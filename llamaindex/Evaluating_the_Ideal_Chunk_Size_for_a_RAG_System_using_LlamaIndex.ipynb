{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FqeieOC5vUB"
      },
      "source": [
        "# Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIvSXj365r2n"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "Retrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the `chunk_size`. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex `Response Evaluation` comes handy. In this blogpost, we'll guide you through the steps to determine the best `chunk size` using LlamaIndex’s `Response Evaluation` module. If you're unfamiliar with the `Response` Evaluation module, we recommend reviewing its [documentation](https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html) before proceeding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpbtWrEa53Ct"
      },
      "source": [
        "## **Why Chunk Size Matters**\n",
        "\n",
        "Choosing the right `chunk_size` is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n",
        "\n",
        "1. **Relevance and Granularity**: A small `chunk_size`, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the `similarity_top_k` setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
        "2. **Response Generation Time**: As the `chunk_size` increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
        "\n",
        "In essence, determining the optimal `chunk_size` is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use-case and dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR8jlf3358_z"
      },
      "source": [
        "## **Setup**\n",
        "\n",
        "Before embarking on the experiment, we need to ensure all requisite modules are imported:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ItNWVKRRD67j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: llama-index in /home/addo/.local/lib/python3.11/site-packages (0.10.7)\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /home/addo/.local/lib/python3.11/site-packages (0.1.3)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.10.7)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.2)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.2)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.3)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
            "  Downloading thinc-8.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (23.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.26.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/addo/.local/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.12.2)\n",
            "Requirement already satisfied: httpx in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.26.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.1.13)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.8.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.1)\n",
            "Requirement already satisfied: pandas in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (9.0.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (4.8.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (0.0.2)\n",
            "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (1.23.24)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/addo/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
            "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: platformdirs in /usr/lib/python3.11/site-packages (from setuptools->spacy) (3.11.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/lib/python3.11/site-packages (from setuptools->spacy) (3.11.1)\n",
            "Requirement already satisfied: more-itertools in /usr/lib/python3.11/site-packages (from setuptools->spacy) (10.1.0)\n",
            "Requirement already satisfied: ordered-set in /usr/lib/python3.11/site-packages (from setuptools->spacy) (4.1.0)\n",
            "Requirement already satisfied: tomli in /usr/lib/python3.11/site-packages (from setuptools->spacy) (2.0.1)\n",
            "Requirement already satisfied: validate-pyproject in /usr/lib/python3.11/site-packages (from setuptools->spacy) (0.13.post1.dev0+gb752273.d20230520)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /home/addo/.local/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n",
            "Requirement already satisfied: anyio in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (4.2.0)\n",
            "Requirement already satisfied: certifi in /usr/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/addo/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (0.14.0)\n",
            "Requirement already satisfied: joblib in /home/addo/.local/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.8.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.22 in /home/addo/.local/lib/python3.11/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (1.23.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/addo/.local/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/addo/.local/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/addo/.local/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.20.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (3.9.0)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (4.3.0)\n",
            "Requirement already satisfied: autocommand in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (2.2.2)\n",
            "Requirement already satisfied: inflect in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/addo/.local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/addo/.local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.4)\n",
            "Requirement already satisfied: fastjsonschema<=3,>=2.16.2 in /usr/lib/python3.11/site-packages (from validate-pyproject->setuptools->spacy) (2.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n",
            "Downloading spacy-3.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.9/490.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.1/920.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\n",
            "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-embeddings-openai spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "y9SVm76h58de"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.core.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "import openai\n",
        "import time\n",
        "openai.api_key = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO21UssT6L8N"
      },
      "source": [
        "## **Load Data**\n",
        "\n",
        "Let’s load our document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "x6QdEBd-17OC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "# Load Data\n",
        "# reader = SimpleDirectoryReader(\"../data/web-software-development-1-0/\", recursive=True)\n",
        "document_base_path = \"../data/web-software-development-1-0/\"\n",
        "documents_path = f\"{document_base_path}13-working-with-databases-i/\"\n",
        "reader = SimpleDirectoryReader(documents_path, recursive=True)\n",
        "\n",
        "documents = reader.load_data()\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnpPtiz56TYA"
      },
      "source": [
        "## **Question Generation**\n",
        "\n",
        "To select the right `chunk_size`, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various `chunk_sizes`. The `DatasetGenerator` will help us generate questions from the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "26BgDF3L6Z0r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of documents:  14\n",
            "=== EVAL QUESTIONS ===\n",
            "['What is the significance of having a database client when working with databases?', 'What is the default database client mentioned in the document for accessing a PostgreSQL database?', 'Where can you find a list of PostgreSQL clients for different operating systems according to the document?']\n"
          ]
        }
      ],
      "source": [
        "# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\n",
        "eval_documents = documents[:20]\n",
        "print(\"Amount of documents: \", len(eval_documents))\n",
        "\n",
        "# data_generator = DatasetGenerator.from_documents(documents)\n",
        "# eval_questions = data_generator.generate_questions_from_nodes(num = 40)\n",
        "\n",
        "# generated from above, hardcoded to save costs\n",
        "all_eval_questions = ['What is the importance of using a database in web applications?',\n",
        "                      'What database management system will be used in this course?',\n",
        "                      'What are the learning objectives related to working with databases?',\n",
        "                      'Where can you find a tutorial for SQL basics if you need a refresher?',\n",
        "                      'How can you start using PostgreSQL according to the document?',\n",
        "                      'What is the recommended approach for taking PostgreSQL into use for development?',\n",
        "                      #   'What is the purpose of the Walking skeleton in relation to PostgreSQL?',\n",
        "                      'What are some options for running PostgreSQL locally?',\n",
        "                      'Name two hosted services that provide PostgreSQL as a service.',\n",
        "                      #   'Why does the document strongly recommend using the first option for development when starting to use PostgreSQL?',\n",
        "                      'What are two options for starting to use PostgreSQL as mentioned in the document?',\n",
        "                      'What are some examples of hosted services that provide PostgreSQL databases?',\n",
        "                      #   'Why does the document strongly recommend using the first option for development?',\n",
        "                      'How can you get started with ElephantSQL according to the document?',\n",
        "                      \"What attributes are included in the table created in the document's example using SQL?\",\n",
        "                      'How can you add names to the table in ElephantSQL according to the document?',\n",
        "                      \"What SQL query can you use to select all rows from the 'names' table in ElephantSQL?\",\n",
        "                      \"What library is used in the document's example to access the database programatically?\",\n",
        "                      #   'What information is grayed out in the image of the ElephantSQL details page?',\n",
        "                      \"What is the purpose of the 'id' attribute in the table created in the document's example?\",\n",
        "                      #   'What library is used in the first example to access a PostgreSQL database in the provided code snippet?',\n",
        "                      #   'How can you specify the database credentials when using Postgres.js in the provided code snippet?',\n",
        "                      #   'What is the purpose of the `max: 2` parameter in the Postgres.js example?',\n",
        "                      #   'In the second example, what library is used to access a PostgreSQL database?',\n",
        "                      'What is the recommended alternative to Deno Postgres mentioned in the document?',\n",
        "                      #   'How can you establish a connection to a PostgreSQL database using Deno Postgres in the provided code snippet?',\n",
        "                      #   'What query is executed in the Deno Postgres example to retrieve data from the database?',\n",
        "                      'What is the significance of having a database client when working with databases?',\n",
        "                      'What is the default database client mentioned in the document for accessing a PostgreSQL database?',\n",
        "                      'Where can you find a list of PostgreSQL clients for different operating systems according to the document?',\n",
        "                      'What database driver is used when working with Deno and PostgreSQL in the provided document?',\n",
        "                      'How can you create a database client using the Postgres.js driver?',\n",
        "                      #   'In the example code provided, what SQL query is being executed to retrieve data from the database?',\n",
        "                      'How does Postgres.js ensure safe query generation when constructing SQL queries?',\n",
        "                      'What is the purpose of the `sql` function in the Postgres.js driver?',\n",
        "                      #   'In the example code, how is the result data iterated over to print only the name property?',\n",
        "                      #   'What SQL statement is used to insert data into a database in the provided document?',\n",
        "                      #   'After inserting a new name into the database, how many names are present in the database according to the output?',\n",
        "                      'What flag is required to be used with Deno when working with the Postgres.js driver?',\n",
        "                      'How can you access the Postgres.js documentation for further details on tagged template literals?']\n",
        "\n",
        "\n",
        "eval_questions = [\n",
        "    'What is the significance of having a database client when working with databases?',\n",
        "    'What is the default database client mentioned in the document for accessing a PostgreSQL database?',\n",
        "    'Where can you find a list of PostgreSQL clients for different operating systems according to the document?',\n",
        "]\n",
        "print(\"=== EVAL QUESTIONS ===\")\n",
        "print(eval_questions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WwA-0N6dMO"
      },
      "source": [
        "## Setting Up Evaluators\n",
        "\n",
        "We are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, `FaithfulnessEvaluator` and `RelevancyEvaluator`, are initialised with the `service_context` .\n",
        "\n",
        "1. **Faithfulness Evaluator** - It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.\n",
        "2. **Relevancy Evaluator** - It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "G2LoMRtr6fnG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_177645/1846307701.py:8: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context_gpt = ServiceContext.from_defaults(llm=llm_evaluate)\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Settings\n",
        "# We will use GPT-4 for evaluating the responses\n",
        "\n",
        "llm_evaluate = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
        "\n",
        "# Define service context for llm evaluation\n",
        "service_context_gpt = ServiceContext.from_defaults(llm=llm_evaluate)\n",
        "\n",
        "# Define Faithfulness and Relevancy Evaluators\n",
        "faithfulness_gpt = FaithfulnessEvaluator(service_context=service_context_gpt)\n",
        "relevancy_gpt = RelevancyEvaluator(service_context=service_context_gpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (DO NOT RUN) Debugging local embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from llama_index.core.schema import IndexNode\n",
        "from llama_index.core import (\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.retrievers import RecursiveRetriever\n",
        "import os\n",
        "# from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "\n",
        "\n",
        "def build_index_local(docs, chunk_size, out_path: str):\n",
        "    print(\"Chunk size: \", chunk_size)\n",
        "\n",
        "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",\n",
        "                                  chunk_size=chunk_size,\n",
        "                                  )\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    nodes = []\n",
        "\n",
        "    splitter = SentenceSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_size/4)\n",
        "    for idx, doc in enumerate(docs):\n",
        "        print('Splitting: ' + str(idx))\n",
        "\n",
        "        cur_nodes = splitter.get_nodes_from_documents([doc])\n",
        "        for cur_node in cur_nodes:\n",
        "            # ID will be base + parent\n",
        "            file_path = doc.metadata[\"file_path\"].split(document_base_path)[1]\n",
        "            new_node = IndexNode(\n",
        "                text=cur_node.text or \"None\",\n",
        "                index_id=str(file_path),\n",
        "                metadata=doc.metadata,\n",
        "                # obj=doc\n",
        "            )\n",
        "            nodes.append(new_node)\n",
        "        \n",
        "\n",
        "        # Debugging\n",
        "        print(len(cur_nodes), len(str(doc)), len(str(cur_nodes[0])))\n",
        "        for xyz in cur_nodes:\n",
        "            print(xyz)\n",
        "            print(\"-\")\n",
        "        print()\n",
        "        print(\"----DOC-----\")\n",
        "        print(doc)\n",
        "\n",
        "        print()\n",
        "        print()\n",
        "\n",
        "    print(\"num nodes: \" + str(len(nodes)))\n",
        "\n",
        "    service_context = ServiceContext.from_defaults(\n",
        "        llm=llm_evaluate, embed_model=embed_model)\n",
        "\n",
        "    # save index to disk\n",
        "    if not os.path.exists(out_path):\n",
        "        index = VectorStoreIndex(nodes, service_context=service_context)\n",
        "        index.set_index_id(\"simple_index\")\n",
        "        index.storage_context.persist(f\"./{out_path}\")\n",
        "    else:\n",
        "        # rebuild storage context\n",
        "        storage_context = StorageContext.from_defaults(\n",
        "            persist_dir=f\"./{out_path}\"\n",
        "        )\n",
        "        # load index\n",
        "        index = load_index_from_storage(\n",
        "            storage_context, index_id=\"simple_index\", service_context=service_context\n",
        "            # storage_context, index_id=\"simple_index\", embed_model=embed_model\n",
        "        )\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "# build_index_local(eval_documents, 1024, \"Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# vs. Storing embeddings (Weaviate)\n",
        "`docker-compose -f docker-compose.weaviate-persistent.yml up`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/addo/.local/lib/python3.11/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.5.0.\n",
            "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from llama_index.core.schema import IndexNode\n",
        "from llama_index.core import (\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.retrievers import RecursiveRetriever\n",
        "import os\n",
        "# from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "\n",
        "import weaviate\n",
        "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# local\n",
        "client = weaviate.Client(\"http://localhost:8080\")\n",
        "\n",
        "\n",
        "def get_filepath_substring(file_path):\n",
        "    return file_path.split(document_base_path)[1]\n",
        "\n",
        "\n",
        "def build_index(docs, chunk_size):\n",
        "    print(\"Chunk size: \", chunk_size)\n",
        "\n",
        "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",\n",
        "                                  chunk_size=chunk_size,\n",
        "                                  )\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    service_context = ServiceContext.from_defaults(\n",
        "        llm=llm_evaluate, embed_model=embed_model)\n",
        "\n",
        "    index_name = f\"NB{chunk_size}\"\n",
        "\n",
        "    # save index to disk if does not exist\n",
        "    if not client.schema.exists(index_name):\n",
        "        print(\"Schema does not exist, rebuilding and then storing in db\")\n",
        "        nodes = []\n",
        "        splitter = SentenceSplitter(\n",
        "            chunk_size=chunk_size, chunk_overlap=chunk_size/4)\n",
        "        for idx, doc in enumerate(docs):\n",
        "            print('Splitting: ' + str(idx))\n",
        "\n",
        "            cur_nodes = splitter.get_nodes_from_documents([doc])\n",
        "            for cur_node in cur_nodes:\n",
        "                # ID will be base + parent\n",
        "                file_path = get_filepath_substring(doc.metadata[\"file_path\"])\n",
        "                new_node = IndexNode(\n",
        "                    text=cur_node.text or \"None\",\n",
        "                    index_id=str(file_path),\n",
        "                    metadata=doc.metadata,\n",
        "                    # obj=doc\n",
        "                )\n",
        "                nodes.append(new_node)\n",
        "\n",
        "        print(\"num nodes: \" + str(len(nodes)))\n",
        "\n",
        "        vector_store = WeaviateVectorStore(\n",
        "            weaviate_client=client, index_name=index_name\n",
        "        )\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "        index = VectorStoreIndex(nodes, storage_context = storage_context, service_context=service_context)\n",
        "    else:\n",
        "        # load index\n",
        "        print(\"Schema exists already, load cached from database\")\n",
        "        vector_store = WeaviateVectorStore(\n",
        "            weaviate_client=client, index_name=index_name\n",
        "        )\n",
        "        index = VectorStoreIndex.from_vector_store(vector_store, service_context=service_context)\n",
        "\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_lst = [(128,10,True), (128,8,True), (256,6,True), (512,4,True), (1024,2,True), (2048, 1, True)]\n",
        "model_lst = [(128,8,True), (256,6,True), (512,4,True)]\n",
        "\n",
        "n_runs = len(model_lst)\n",
        "\n",
        "answers = [[] for _ in range(n_runs)]\n",
        "sources = [[] for _ in range(n_runs)]\n",
        "\n",
        "time_scores = [[] for _ in range(n_runs)]\n",
        "faithfulness_scores = [[] for _ in range(n_runs)]\n",
        "relevancy_scores = [[] for _ in range(n_runs)]\n",
        "\n",
        "faithfulness_false = [[] for _ in range(n_runs)]\n",
        "relevancy_false = [[] for _ in range(n_runs)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk size:  128\n",
            "Schema exists already, load cached from database\n",
            "Chunk size:  256\n",
            "Schema exists already, load cached from database\n",
            "Chunk size:  512\n",
            "Schema exists already, load cached from database\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_177645/2146967170.py:37: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(\n"
          ]
        }
      ],
      "source": [
        "vector_indices = []\n",
        "\n",
        "def build_all_indices():\n",
        "    for chunk_size, k, is_hybrid in model_lst:\n",
        "        vector_indices.append(build_index(eval_documents, chunk_size))\n",
        "\n",
        "build_all_indices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUncIIxR6gVz"
      },
      "source": [
        "## **Response Evaluation For A Chunk Size**\n",
        "\n",
        "We evaluate each chunk_size based on 3 metrics.\n",
        "\n",
        "1. Average Response Time.\n",
        "2. Average Faithfulness.\n",
        "3. Average Relevancy.\n",
        "\n",
        "Here's a function, `evaluate_response_time_and_accuracy`, that does just that which has:\n",
        "\n",
        "1. VectorIndex Creation.\n",
        "2. Building the Query Engine**.**\n",
        "3. Metrics Calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper methods\n",
        "\n",
        "def build_query_engine(vector_index, similarity_top_k, is_hybrid):\n",
        "    print(\"== BUILDING QUERY ENGINE ==\")\n",
        "    if is_hybrid:\n",
        "        query_engine = vector_index.as_query_engine(\n",
        "            similarity_top_k=similarity_top_k, embed_model=Settings.embed_model,\n",
        "            vector_store_query_mode=\"hybrid\", alpha=0.0  # BM25\n",
        "        )\n",
        "        return query_engine\n",
        "    # -- VEC ONLY --\n",
        "    query_engine = vector_index.as_query_engine(\n",
        "        similarity_top_k=similarity_top_k, embed_model=Settings.embed_model,\n",
        "    )\n",
        "    return query_engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "dEC2Lr0z6p1N"
      },
      "outputs": [],
      "source": [
        "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
        "# We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
        "\n",
        "\n",
        "def evaluate_response_time_and_accuracy(chunk_size, similarity_top_k, run_i, eval_questions=eval_questions, label=\"default\", is_hybrid=True):\n",
        "    \"\"\"\n",
        "    Evaluate the average response time, faithfulness, and relevancy of responses for a given chunk size.\n",
        "    \"\"\"\n",
        "\n",
        "    vector_index = vector_indices[run_i]\n",
        "\n",
        "    # Build query engine\n",
        "    query_engine = build_query_engine(vector_index, similarity_top_k, is_hybrid)\n",
        "\n",
        "\n",
        "    # Iterate over each question in eval_questions to compute metrics.\n",
        "    # print(\"=========== QA pairs\")\n",
        "    for question in eval_questions:\n",
        "        print(\"--Q: \", question)\n",
        "        start_time = time.time()\n",
        "        response_vector = query_engine.query(question)\n",
        "        print(\"--A: \", str(response_vector))\n",
        "        \n",
        "        print(\"--sources: \")\n",
        "        # get unique list of file paths of source docs\n",
        "        raw_file_paths = list(set(value['file_path'] for value in response_vector.metadata.values()))\n",
        "        source_file_paths = list(map(get_filepath_substring, raw_file_paths))\n",
        "        print(source_file_paths)\n",
        "        sources[run_i].append(source_file_paths)\n",
        "        print(\"----------\")\n",
        "        # print(\"---documents: \")\n",
        "        # print(str(response_vector.get_formatted_sources(length=1000)))\n",
        "        # print(\"----------\")\n",
        "\n",
        "\n",
        "        answers[run_i].append(response_vector)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        faithfulness_result = faithfulness_gpt.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        relevancy_result = relevancy_gpt.evaluate_response(\n",
        "            query=question, response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        if not faithfulness_result:\n",
        "            faithfulness_false[run_i].append((question, str(response_vector)))\n",
        "        if not relevancy_result:\n",
        "            relevancy_false[run_i].append((question, str(response_vector)))\n",
        "\n",
        "        time_scores[run_i].append(elapsed_time)\n",
        "        faithfulness_scores[run_i].append(faithfulness_result)\n",
        "        relevancy_scores[run_i].append(relevancy_result)\n",
        "\n",
        "        print(\n",
        "            f\"t={elapsed_time}, f={faithfulness_result}, r={relevancy_result}\\n-------\")\n",
        "\n",
        "    print(\"===========\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# from llama_index.core.base.response.schema import Response\n",
        "\n",
        "# Settings.llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
        "\n",
        "# def evaluate_response_time_and_accuracy_without_rag(eval_questions=eval_questions):\n",
        "#     \"\"\"\n",
        "#     Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
        "\n",
        "#     Parameters:\n",
        "#     chunk_size (int): The size of data chunks being processed.\n",
        "\n",
        "#     Returns:\n",
        "#     tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
        "#     \"\"\"\n",
        "\n",
        "#     total_response_time = 0\n",
        "#     total_faithfulness = 0\n",
        "#     total_relevancy = 0\n",
        "\n",
        "#     # By default, similarity_top_k is set to 2. To experiment with different values, pass it as an argument to as_query_engine()\n",
        "#     num_questions = len(eval_questions)\n",
        "\n",
        "#     # index = VectorStoreIndex(nodes=[])\n",
        "#     # query_engine = index.as_query_engine(llm=OpenAI())\n",
        "\n",
        "\n",
        "#     # Iterate over each question in eval_questions to compute metrics.\n",
        "#     # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
        "#     # we're using a loop here to specifically measure response time for different chunk sizes.\n",
        "#     print(\"=========== QA pairs\")\n",
        "#     for question in eval_questions:\n",
        "#         print(\"--Q\\n\", question, \"\\n--\")\n",
        "#         start_time = time.time()\n",
        "\n",
        "#         response = str(OpenAI().complete(question))\n",
        "#         response_vector = Response(response)\n",
        "#         # response_vector = query_engine.query(question)\n",
        "\n",
        "#         print(\"--A\\n\", response, \"\\n--\")\n",
        "#         elapsed_time = time.time() - start_time\n",
        "\n",
        "#         faithfulness_result = faithfulness_gpt.evaluate_response(\n",
        "#             response=response_vector\n",
        "#         ).passing\n",
        "\n",
        "#         relevancy_result = relevancy_gpt.evaluate_response(\n",
        "#             query=question, response=response_vector\n",
        "#         ).passing\n",
        "\n",
        "#         total_response_time += elapsed_time\n",
        "#         total_faithfulness += faithfulness_result\n",
        "#         total_relevancy += relevancy_result\n",
        "\n",
        "#         # TODO: both response and retrieval evaluation\n",
        "\n",
        "#     print(\"===========\")\n",
        "#     average_response_time = total_response_time / num_questions\n",
        "#     average_faithfulness = total_faithfulness / num_questions\n",
        "#     average_relevancy = total_relevancy / num_questions\n",
        "\n",
        "#     return average_response_time, average_faithfulness, average_relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8DQvTP96s48"
      },
      "source": [
        "## **Testing Across Different Chunk Sizes**\n",
        "\n",
        "We'll evaluate a range of chunk sizes to identify which offers the most promising metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "jlKICwXH6Tib"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== BUILDING QUERY ENGINE ==\n",
            "--Q:  What is the significance of having a database client when working with databases?\n",
            "--A:  Having a database client is significant when working with databases because it allows the application to establish a connection to the database, execute queries to retrieve or manipulate data, and manage transactions effectively. The database client facilitates communication between the application and the database server, enabling seamless interaction with the database through functions like connecting, querying, and closing connections. This ensures efficient data retrieval, storage, and management within the database system.\n",
            "--sources: \n",
            "['13-working-with-databases-i/5-deno-postgres-querying-a-database.mdx']\n",
            "----------\n",
            "t=2.497727394104004, f=True, r=True\n",
            "-------\n",
            "--Q:  What is the default database client mentioned in the document for accessing a PostgreSQL database?\n",
            "--A:  The default database client mentioned in the document for accessing a PostgreSQL database is \"Client\".\n",
            "--sources: \n",
            "['13-working-with-databases-i/5-deno-postgres-querying-a-database.mdx', '13-working-with-databases-i/1-getting-started.mdx']\n",
            "----------\n",
            "t=1.295851707458496, f=True, r=False\n",
            "-------\n",
            "--Q:  Where can you find a list of PostgreSQL clients for different operating systems according to the document?\n",
            "--A:  You can find a list of PostgreSQL clients for different operating systems on the PostgreSQL wiki page dedicated to PostgreSQL Clients.\n",
            "--sources: \n",
            "['13-working-with-databases-i/1-getting-started.mdx']\n",
            "----------\n",
            "t=2.799791097640991, f=True, r=True\n",
            "-------\n",
            "===========\n",
            "== BUILDING QUERY ENGINE ==\n",
            "--Q:  What is the significance of having a database client when working with databases?\n",
            "--A:  Having a database client is significant when working with databases because it allows the application to establish a connection to the database, execute queries, and retrieve results. The database client handles the communication between the application and the database server, enabling the application to interact with the database by sending SQL queries and receiving the corresponding data. This interaction is essential for performing operations such as inserting, updating, deleting, and querying data within the database.\n",
            "--sources: \n",
            "['13-working-with-databases-i/5-deno-postgres-querying-a-database.mdx', '13-working-with-databases-i/1-getting-started.mdx']\n",
            "----------\n",
            "t=2.550724506378174, f=True, r=True\n",
            "-------\n",
            "--Q:  What is the default database client mentioned in the document for accessing a PostgreSQL database?\n",
            "--A:  The default database client mentioned in the document for accessing a PostgreSQL database is the `psql` console.\n",
            "--sources: \n",
            "['13-working-with-databases-i/1-getting-started.mdx']\n",
            "----------\n",
            "t=1.7164900302886963, f=True, r=True\n",
            "-------\n",
            "--Q:  Where can you find a list of PostgreSQL clients for different operating systems according to the document?\n",
            "--A:  The PostgreSQL wiki provides a list of PostgreSQL clients for different operating systems.\n",
            "--sources: \n",
            "['13-working-with-databases-i/1-getting-started.mdx']\n",
            "----------\n",
            "t=7.933391094207764, f=True, r=True\n",
            "-------\n",
            "===========\n",
            "== BUILDING QUERY ENGINE ==\n",
            "--Q:  What is the significance of having a database client when working with databases?\n",
            "--A:  Having a database client when working with databases is significant as it provides a user interface or tool that allows users to interact with the database. These clients offer functionalities such as executing queries, managing database objects, viewing data, and monitoring database performance. They simplify the process of interacting with databases and make it more user-friendly for developers and administrators.\n",
            "--sources: \n",
            "['13-working-with-databases-i/3-query-parameters.mdx', '13-working-with-databases-i/1-getting-started.mdx']\n",
            "----------\n",
            "t=1.766951084136963, f=True, r=True\n",
            "-------\n",
            "--Q:  What is the default database client mentioned in the document for accessing a PostgreSQL database?\n",
            "--A:  The default database client mentioned in the document for accessing a PostgreSQL database is the `psql` console, which provides terminal access to the database.\n",
            "--sources: \n",
            "['13-working-with-databases-i/1-getting-started.mdx']\n",
            "----------\n",
            "t=1.4078822135925293, f=True, r=False\n",
            "-------\n",
            "--Q:  Where can you find a list of PostgreSQL clients for different operating systems according to the document?\n",
            "--A:  You can find a list of PostgreSQL clients for different operating systems on the PostgreSQL wiki page dedicated to PostgreSQL Clients.\n",
            "--sources: \n",
            "['13-working-with-databases-i/1-getting-started.mdx']\n",
            "----------\n",
            "t=1.4740664958953857, f=True, r=True\n",
            "-------\n",
            "===========\n"
          ]
        }
      ],
      "source": [
        "from statistics import mean\n",
        "\n",
        "# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n",
        "for run_i, (chunk_size, similarity_top_k, is_hybrid) in enumerate(model_lst):\n",
        "    evaluate_response_time_and_accuracy(chunk_size, similarity_top_k, run_i, eval_questions=eval_questions, is_hybrid=is_hybrid)\n",
        "\n",
        "response_model_name = Settings.llm.model\n",
        "evaluation_model_name = llm_evaluate.model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= STATS ============\n",
            "n_questions: 3\n",
            "response model: gpt-3.5-turbo\n",
            "evaluation model: gpt-3.5-turbo\n",
            "============= MODELS ===========\n",
            "(hybr-gpt-3.5-turbo-128*10) - avg res time: 2.20s, avg faithfulness: 1.00, avg relevancy: 0.67\n",
            "(hybr-gpt-3.5-turbo-128*8) - avg res time: 4.07s, avg faithfulness: 1.00, avg relevancy: 1.00\n",
            "(hybr-gpt-3.5-turbo-256*6) - avg res time: 1.55s, avg faithfulness: 1.00, avg relevancy: 0.67\n",
            "============= QA ============\n",
            "========================================================\n",
            "---Q evaluated by gpt-3.5-turbo: What is the significance of having a database client when working with databases?\n",
            "===\n",
            "(hybr-gpt-3.5-turbo-128*10): Having a database client is significant when working with databases because it allows the application to establish a connection to the database, execute queries to retrieve or manipulate data, and manage transactions effectively. The database client facilitates communication between the application and the database server, enabling seamless interaction with the database through functions like connecting, querying, and closing connections. This ensures efficient data retrieval, storage, and management within the database system.\n",
            "t=2.497727394104004s f=True r=True\n",
            "sources: ['13-working-with-databases-i/5-deno-postgres-querying-a-database.mdx']\n",
            "===\n",
            "(hybr-gpt-3.5-turbo-128*8): Having a database client is significant when working with databases because it allows the application to establish a connection to the database, execute queries, and retrieve results. The database client handles the communication between the application and the database server, enabling the application to interact with the database by sending SQL queries and receiving the corresponding data. This interaction is essential for performing operations such as inserting, updating, deleting, and querying data within the database.\n",
            "t=2.550724506378174s f=True r=True\n",
            "sources: ['13-working-with-databases-i/5-deno-postgres-querying-a-database.mdx', '13-working-with-databases-i/1-getting-started.mdx']\n",
            "===\n",
            "(hybr-gpt-3.5-turbo-256*6): Having a database client when working with databases is significant as it provides a user interface or tool that allows users to interact with the database. These clients offer functionalities such as executing queries, managing database objects, viewing data, and monitoring database performance. They simplify the process of interacting with databases and make it more user-friendly for developers and administrators.\n",
            "t=1.766951084136963s f=True r=True\n",
            "sources: ['13-working-with-databases-i/3-query-parameters.mdx', '13-working-with-databases-i/1-getting-started.mdx']\n",
            "========================================================\n",
            "---Q evaluated by gpt-3.5-turbo: What is the default database client mentioned in the document for accessing a PostgreSQL database?\n",
            "===\n",
            "(hybr-gpt-3.5-turbo-128*10): The default database client mentioned in the document for accessing a PostgreSQL database is \"Client\".\n",
            "t=1.295851707458496s f=True r=False\n",
            "sources: ['13-working-with-databases-i/5-deno-postgres-querying-a-database.mdx', '13-working-with-databases-i/1-getting-started.mdx']\n",
            "===\n",
            "(hybr-gpt-3.5-turbo-128*8): The default database client mentioned in the document for accessing a PostgreSQL database is the `psql` console.\n",
            "t=1.7164900302886963s f=True r=True\n",
            "sources: ['13-working-with-databases-i/1-getting-started.mdx']\n",
            "===\n",
            "(hybr-gpt-3.5-turbo-256*6): The default database client mentioned in the document for accessing a PostgreSQL database is the `psql` console, which provides terminal access to the database.\n",
            "t=1.4078822135925293s f=True r=False\n",
            "sources: ['13-working-with-databases-i/1-getting-started.mdx']\n",
            "========================================================\n",
            "---Q evaluated by gpt-3.5-turbo: Where can you find a list of PostgreSQL clients for different operating systems according to the document?\n",
            "===\n",
            "(hybr-gpt-3.5-turbo-128*10): You can find a list of PostgreSQL clients for different operating systems on the PostgreSQL wiki page dedicated to PostgreSQL Clients.\n",
            "t=2.799791097640991s f=True r=True\n",
            "sources: ['13-working-with-databases-i/1-getting-started.mdx']\n",
            "===\n",
            "(hybr-gpt-3.5-turbo-128*8): The PostgreSQL wiki provides a list of PostgreSQL clients for different operating systems.\n",
            "t=7.933391094207764s f=True r=True\n",
            "sources: ['13-working-with-databases-i/1-getting-started.mdx']\n",
            "===\n",
            "(hybr-gpt-3.5-turbo-256*6): You can find a list of PostgreSQL clients for different operating systems on the PostgreSQL wiki page dedicated to PostgreSQL Clients.\n",
            "t=1.4740664958953857s f=True r=True\n",
            "sources: ['13-working-with-databases-i/1-getting-started.mdx']\n"
          ]
        }
      ],
      "source": [
        "print(\"============= STATS ============\")\n",
        "print(f\"n_questions: {len(eval_questions)}\")\n",
        "print(f\"response model: {response_model_name}\")\n",
        "print(f\"evaluation model: {evaluation_model_name}\")\n",
        "print(\"============= MODELS ===========\")\n",
        "\n",
        "for model in range(n_runs):\n",
        "    time_avg = mean(time_scores[model])\n",
        "    faithfulness_avg = mean(faithfulness_scores[model])\n",
        "    relevancy_avg = mean(relevancy_scores[model])\n",
        "    print(\n",
        "        f\"(hybr-{response_model_name}-{chunk_sizes[model]}*{similarities_top_k[model]}) - avg res time: {time_avg:.2f}s, avg faithfulness: {faithfulness_avg:.2f}, avg relevancy: {relevancy_avg:.2f}\")\n",
        "\n",
        "\n",
        "print(\"============= QA ============\")\n",
        "\n",
        "for i, question in enumerate(eval_questions):\n",
        "    print(\"========================================================\")\n",
        "    print(f\"---Q evaluated by {evaluation_model_name}: {question}\")\n",
        "    for model in range(n_runs):\n",
        "        print(\"===\")\n",
        "        print(\n",
        "            f\"(hybr-{response_model_name}-{chunk_sizes[model]}*{similarities_top_k[model]}): {answers[model][i]}\")\n",
        "        print(f\"t={time_scores[model][i]}s f={faithfulness_scores[model][i]} r={relevancy_scores[model][i]}\")\n",
        "        print(f\"sources: {sources[model][i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating responses without RAG\n",
        "\n",
        "## Warning\n",
        "From the way that the questions are stated, it might be confusing for the LLM to provide a response to them or impossible to response.\n",
        "For example:\n",
        "- \"What database management system will be used in this course?\"\n",
        "- \"Why does the document strongly recommend using the first option for development when starting to use PostgreSQL?\"\n",
        " \n",
        "These have to be cleaned manually before running the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy_without_rag(eval_questions=eval_questions)\n",
        "# print(f\"Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
