{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FqeieOC5vUB"
      },
      "source": [
        "# Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIvSXj365r2n"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "Retrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the `chunk_size`. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex `Response Evaluation` comes handy. In this blogpost, we'll guide you through the steps to determine the best `chunk size` using LlamaIndex’s `Response Evaluation` module. If you're unfamiliar with the `Response` Evaluation module, we recommend reviewing its [documentation](https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html) before proceeding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpbtWrEa53Ct"
      },
      "source": [
        "## **Why Chunk Size Matters**\n",
        "\n",
        "Choosing the right `chunk_size` is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n",
        "\n",
        "1. **Relevance and Granularity**: A small `chunk_size`, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the `similarity_top_k` setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
        "2. **Response Generation Time**: As the `chunk_size` increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
        "\n",
        "In essence, determining the optimal `chunk_size` is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use-case and dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR8jlf3358_z"
      },
      "source": [
        "## **Setup**\n",
        "\n",
        "Before embarking on the experiment, we need to ensure all requisite modules are imported:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ItNWVKRRD67j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: llama-index in /home/addo/.local/lib/python3.11/site-packages (0.10.7)\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /home/addo/.local/lib/python3.11/site-packages (0.1.3)\n",
            "Requirement already satisfied: spacy in /home/addo/.local/lib/python3.11/site-packages (3.7.4)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.10.7)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.2)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.2)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.26.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/addo/.local/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.12.2)\n",
            "Requirement already satisfied: httpx in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.26.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.1.13)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.8.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.1)\n",
            "Requirement already satisfied: pandas in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (9.0.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (4.8.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (0.0.2)\n",
            "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (1.23.24)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/addo/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/addo/.local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/addo/.local/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/addo/.local/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: platformdirs in /usr/lib/python3.11/site-packages (from setuptools->spacy) (3.11.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/lib/python3.11/site-packages (from setuptools->spacy) (3.11.1)\n",
            "Requirement already satisfied: more-itertools in /usr/lib/python3.11/site-packages (from setuptools->spacy) (10.1.0)\n",
            "Requirement already satisfied: ordered-set in /usr/lib/python3.11/site-packages (from setuptools->spacy) (4.1.0)\n",
            "Requirement already satisfied: tomli in /usr/lib/python3.11/site-packages (from setuptools->spacy) (2.0.1)\n",
            "Requirement already satisfied: validate-pyproject in /usr/lib/python3.11/site-packages (from setuptools->spacy) (0.13.post1.dev0+gb752273.d20230520)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /home/addo/.local/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n",
            "Requirement already satisfied: anyio in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (4.2.0)\n",
            "Requirement already satisfied: certifi in /usr/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/addo/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (0.14.0)\n",
            "Requirement already satisfied: joblib in /home/addo/.local/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.8.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.22 in /home/addo/.local/lib/python3.11/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (1.23.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/addo/.local/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/addo/.local/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/addo/.local/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.20.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (3.9.0)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (4.3.0)\n",
            "Requirement already satisfied: autocommand in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (2.2.2)\n",
            "Requirement already satisfied: inflect in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/addo/.local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/addo/.local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.4)\n",
            "Requirement already satisfied: fastjsonschema<=3,>=2.16.2 in /usr/lib/python3.11/site-packages (from validate-pyproject->setuptools->spacy) (2.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-embeddings-openai spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "y9SVm76h58de"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.core.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator,\n",
        "    CorrectnessEvaluator\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "import openai\n",
        "import time\n",
        "openai.api_key = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO21UssT6L8N"
      },
      "source": [
        "## **Load Data**\n",
        "\n",
        "Let’s load our document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "x6QdEBd-17OC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "319\n"
          ]
        }
      ],
      "source": [
        "# Load Data\n",
        "document_base_path = \"../data/web-software-development/\"\n",
        "\n",
        "# documents_path = f\"{document_base_path}21-working-with-databases/\" # single chapter\n",
        "documents_path = f\"{document_base_path}\" # full course\n",
        "\n",
        "reader = SimpleDirectoryReader(documents_path, recursive=True)\n",
        "\n",
        "documents = reader.load_data()\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnpPtiz56TYA"
      },
      "source": [
        "## **Question Generation**\n",
        "\n",
        "To select the right `chunk_size`, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various `chunk_sizes`. The `DatasetGenerator` will help us generate questions from the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "26BgDF3L6Z0r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of documents:  319\n",
            "=== EVAL QUESTIONS AND ANSWERS ===\n",
            "[('Why use NoSQL instead of SQL?', 'They are designed to scale horizontally.'), ('What is the difference between authentication and authorization?', 'The term authentication refers to identifying a user. The term authorization refers to the process of verifying that the user has the rights to perform the actions that the user is trying to perform.'), ('What is a UUID?', 'UUIDs are a common way of identifying resources. They are 128-bit numbers that are designed for being unique without central coordination (i.e. a service that would keep track of which identifier to assign next).')]\n"
          ]
        }
      ],
      "source": [
        "# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\n",
        "# eval_documents = documents[:20]\n",
        "eval_documents = documents\n",
        "print(\"Amount of documents: \", len(eval_documents))\n",
        "\n",
        "# data_generator = DatasetGenerator.from_documents(documents)\n",
        "# eval_questions = data_generator.generate_questions_from_nodes(num = 40)\n",
        "\n",
        "# generated from above, hardcoded to save costs\n",
        "\n",
        "\n",
        "# TODO not representative of student queries\n",
        "# too generic / not comprehension\n",
        "# get data from chatbot\n",
        "\n",
        "\n",
        "gen_eval_questions = ['what is the importance of using a database in web applications?',\n",
        "                      'What database management system will be used in this course?',\n",
        "                      'What are the learning objectives related to working with databases?',\n",
        "                      'Where can you find a tutorial for SQL basics if you need a refresher?',\n",
        "                      'How can you start using PostgreSQL according to the document?',\n",
        "                      'What is the recommended approach for taking PostgreSQL into use for development?',\n",
        "                      #   'What is the purpose of the Walking skeleton in relation to PostgreSQL?',\n",
        "                      'What are some options for running PostgreSQL locally?',\n",
        "                      'Name two hosted services that provide PostgreSQL as a service.',\n",
        "                      #   'Why does the document strongly recommend using the first option for development when starting to use PostgreSQL?',\n",
        "                      'What are two options for starting to use PostgreSQL as mentioned in the document?',\n",
        "                      'What are some examples of hosted services that provide PostgreSQL databases?',\n",
        "                      #   'Why does the document strongly recommend using the first option for development?',\n",
        "                      'How can you get started with ElephantSQL according to the document?',\n",
        "                      \"What attributes are included in the table created in the document's example using SQL?\",\n",
        "                      'How can you add names to the table in ElephantSQL according to the document?',\n",
        "                      \"What SQL query can you use to select all rows from the 'names' table in ElephantSQL?\",\n",
        "                      \"What library is used in the document's example to access the database programatically?\",\n",
        "                      #   'What information is grayed out in the image of the ElephantSQL details page?',\n",
        "                      \"What is the purpose of the 'id' attribute in the table created in the document's example?\",\n",
        "                      #   'What library is used in the first example to access a PostgreSQL database in the provided code snippet?',\n",
        "                      #   'How can you specify the database credentials when using Postgres.js in the provided code snippet?',\n",
        "                      #   'What is the purpose of the `max: 2` parameter in the Postgres.js example?',\n",
        "                      #   'In the second example, what library is used to access a PostgreSQL database?',\n",
        "                      'What is the recommended alternative to Deno Postgres mentioned in the document?',\n",
        "                      #   'How can you establish a connection to a PostgreSQL database using Deno Postgres in the provided code snippet?',\n",
        "                      #   'What query is executed in the Deno Postgres example to retrieve data from the database?',\n",
        "                      'What is the significance of having a database client when working with databases?',\n",
        "                      'What is the default database client mentioned in the document for accessing a PostgreSQL database?',\n",
        "                      'Where can you find a list of PostgreSQL clients for different operating systems according to the document?',\n",
        "                      'What database driver is used when working with Deno and PostgreSQL in the provided document?',\n",
        "                      'How can you create a database client using the Postgres.js driver?',\n",
        "                      #   'In the example code provided, what SQL query is being executed to retrieve data from the database?',\n",
        "                      'How does Postgres.js ensure safe query generation when constructing SQL queries?',\n",
        "                      'What is the purpose of the `sql` function in the Postgres.js driver?',\n",
        "                      #   'In the example code, how is the result data iterated over to print only the name property?',\n",
        "                      #   'What SQL statement is used to insert data into a database in the provided document?',\n",
        "                      #   'After inserting a new name into the database, how many names are present in the database according to the output?',\n",
        "                      'What flag is required to be used with Deno when working with the Postgres.js driver?',\n",
        "                      'How can you access the Postgres.js documentation for further details on tagged template literals?']\n",
        "\n",
        "\n",
        "eval_qa = [\n",
        "    ('Why use NoSQL instead of SQL?',\n",
        "     'They are designed to scale horizontally.'),\n",
        "\n",
        "    ('What is the difference between authentication and authorization?',\n",
        "     'The term authentication refers to identifying a user. The term authorization refers to the process of verifying that the user has the rights to perform the actions that the user is trying to perform.'),\n",
        "\n",
        "    ('What is a UUID?',\n",
        "     'UUIDs are a common way of identifying resources. They are 128-bit numbers that are designed for being unique without central coordination (i.e. a service that would keep track of which identifier to assign next).'),\n",
        "]\n",
        "\n",
        "\n",
        "print(\"=== EVAL QUESTIONS AND ANSWERS ===\")\n",
        "print(eval_qa)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WwA-0N6dMO"
      },
      "source": [
        "## Setting Up Evaluators\n",
        "\n",
        "We are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, `FaithfulnessEvaluator` and `RelevancyEvaluator`, are initialised with the `service_context` .\n",
        "\n",
        "1. **Faithfulness Evaluator** - It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.\n",
        "2. **Relevancy Evaluator** - It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "G2LoMRtr6fnG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_317535/3660369230.py:11: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context_gpt_35 = ServiceContext.from_defaults(llm=llm_evaluate)\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "# We will use for evaluating the responses\n",
        "\n",
        "llm_evaluate = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "gpt35 = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
        "# gpt4 = OpenAI(model=\"gpt-4\", temperature=0.2)\n",
        "\n",
        "# Define service context for llm evaluation\n",
        "service_context_gpt_35 = ServiceContext.from_defaults(llm=llm_evaluate)\n",
        "\n",
        "# Define Faithfulness and Relevancy Evaluators\n",
        "faithfulness_gpt = FaithfulnessEvaluator(\n",
        "    service_context=service_context_gpt_35)\n",
        "relevancy_gpt = RelevancyEvaluator(service_context=service_context_gpt_35)\n",
        "correctness_gpt = CorrectnessEvaluator(service_context=service_context_gpt_35)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (DO NOT RUN) Debugging local embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "# from llama_index.core import Settings\n",
        "\n",
        "# from llama_index.core.schema import IndexNode\n",
        "# from llama_index.core import (\n",
        "#     load_index_from_storage,\n",
        "#     StorageContext,\n",
        "#     VectorStoreIndex,\n",
        "# )\n",
        "# from llama_index.core.node_parser import SentenceSplitter\n",
        "# from llama_index.core import SummaryIndex\n",
        "# from llama_index.core.retrievers import RecursiveRetriever\n",
        "# import os\n",
        "# # from tqdm.notebook import tqdm\n",
        "# import pickle\n",
        "\n",
        "\n",
        "# def build_index_local(docs, chunk_size, out_path: str):\n",
        "#     print(\"Chunk size: \", chunk_size)\n",
        "\n",
        "#     embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",\n",
        "#                                   chunk_size=chunk_size,\n",
        "#                                   )\n",
        "#     Settings.embed_model = embed_model\n",
        "\n",
        "#     nodes = []\n",
        "\n",
        "#     splitter = SentenceSplitter(\n",
        "#         chunk_size=chunk_size, chunk_overlap=chunk_size/4)\n",
        "#     for idx, doc in enumerate(docs):\n",
        "#         print('Splitting: ' + str(idx))\n",
        "\n",
        "#         cur_nodes = splitter.get_nodes_from_documents([doc])\n",
        "#         for cur_node in cur_nodes:\n",
        "#             # ID will be base + parent\n",
        "#             file_path = doc.metadata[\"file_path\"].split(document_base_path)[1]\n",
        "#             new_node = IndexNode(\n",
        "#                 text=cur_node.text or \"None\",\n",
        "#                 index_id=str(file_path),\n",
        "#                 metadata=doc.metadata,\n",
        "#                 # obj=doc\n",
        "#             )\n",
        "#             nodes.append(new_node)\n",
        "        \n",
        "\n",
        "#         # Debugging\n",
        "#         print(len(cur_nodes), len(str(doc)), len(str(cur_nodes[0])))\n",
        "#         for xyz in cur_nodes:\n",
        "#             print(xyz)\n",
        "#             print(\"-\")\n",
        "#         print()\n",
        "#         print(\"----DOC-----\")\n",
        "#         print(doc)\n",
        "\n",
        "#         print()\n",
        "#         print()\n",
        "\n",
        "#     print(\"num nodes: \" + str(len(nodes)))\n",
        "\n",
        "#     service_context = ServiceContext.from_defaults(\n",
        "#         llm=llm_evaluate, embed_model=embed_model)\n",
        "\n",
        "#     # save index to disk\n",
        "#     if not os.path.exists(out_path):\n",
        "#         index = VectorStoreIndex(nodes, service_context=service_context)\n",
        "#         index.set_index_id(\"simple_index\")\n",
        "#         index.storage_context.persist(f\"./{out_path}\")\n",
        "#     else:\n",
        "#         # rebuild storage context\n",
        "#         storage_context = StorageContext.from_defaults(\n",
        "#             persist_dir=f\"./{out_path}\"\n",
        "#         )\n",
        "#         # load index\n",
        "#         index = load_index_from_storage(\n",
        "#             storage_context, index_id=\"simple_index\", service_context=service_context\n",
        "#             # storage_context, index_id=\"simple_index\", embed_model=embed_model\n",
        "#         )\n",
        "\n",
        "#     return index\n",
        "\n",
        "\n",
        "# # build_index_local(eval_documents, 1024, \"Test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# vs. Storing embeddings (Weaviate)\n",
        "`docker-compose -f docker-compose.weaviate-persistent.yml up`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_full_course_index(chunk_size):\n",
        "    return f\"F{chunk_size}\"\n",
        "\n",
        "# If you want to work with different data, create another method with a different index name\n",
        "# in order to not override the old data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/addo/.local/lib/python3.11/site-packages/weaviate/warnings.py:121: DeprecationWarning: Dep005: You are using weaviate-client version 3.26.2. The latest version is 4.5.1.\n",
            "            Please consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from llama_index.core.schema import IndexNode\n",
        "from llama_index.core import (\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.retrievers import RecursiveRetriever\n",
        "import os\n",
        "# from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "\n",
        "import weaviate\n",
        "from llama_index.vector_stores.weaviate import WeaviateVectorStore\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "client = weaviate.Client(\"http://localhost:8080\")\n",
        "\n",
        "\n",
        "def get_filepath_substring(file_path):\n",
        "    return file_path.split(document_base_path)[1]\n",
        "\n",
        "\n",
        "def build_index(docs, chunk_size):\n",
        "    print(\"Chunk size: \", chunk_size)\n",
        "\n",
        "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",\n",
        "                                  chunk_size=chunk_size,\n",
        "                                  )\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    service_context = ServiceContext.from_defaults(\n",
        "        llm=llm_evaluate, embed_model=embed_model)\n",
        "\n",
        "    index_name = get_full_course_index(chunk_size)\n",
        "\n",
        "    # save index to disk if does not exist\n",
        "    if not client.schema.exists(index_name):\n",
        "        print(\"Schema does not exist, rebuilding and then storing in db\")\n",
        "        nodes = []\n",
        "        splitter = SentenceSplitter(\n",
        "            chunk_size=chunk_size, chunk_overlap=chunk_size/4)\n",
        "        for idx, doc in enumerate(docs):\n",
        "            print('Splitting: ' + str(idx))\n",
        "\n",
        "            cur_nodes = splitter.get_nodes_from_documents([doc])\n",
        "            for cur_node in cur_nodes:\n",
        "                # ID will be base + parent\n",
        "                file_path = get_filepath_substring(doc.metadata[\"file_path\"])\n",
        "                new_node = IndexNode(\n",
        "                    text=cur_node.text or \"None\",\n",
        "                    index_id=str(file_path),\n",
        "                    metadata=doc.metadata,\n",
        "                    # obj=doc\n",
        "                )\n",
        "                nodes.append(new_node)\n",
        "\n",
        "        print(\"num nodes: \" + str(len(nodes)))\n",
        "\n",
        "        vector_store = WeaviateVectorStore(\n",
        "            weaviate_client=client, index_name=index_name\n",
        "        )\n",
        "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "        index = VectorStoreIndex(nodes, storage_context = storage_context, service_context=service_context)\n",
        "    else:\n",
        "        # load index\n",
        "        print(\"Schema exists already, load cached from database\")\n",
        "        vector_store = WeaviateVectorStore(\n",
        "            weaviate_client=client, index_name=index_name\n",
        "        )\n",
        "        index = VectorStoreIndex.from_vector_store(vector_store, service_context=service_context)\n",
        "\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# <chunk_size>, <k>, <use_hybrid>, <model>, <use_rag>\n",
        "\n",
        "# config_lst = [(128, 10, True, gpt35, True), (128, 8, True, gpt35, True), (256, 6, True, gpt35, True),\n",
        "#  (512, 4, True, gpt35, True), (1024, 2, True, gpt35, True), (2048, 1, True, gpt35, True)]\n",
        "\n",
        "config_lst = [(128, 8, True, gpt35, True), (128, 8, False,\n",
        "                                            gpt35, True), (128, 8, True, gpt35, False)]\n",
        "\n",
        "n_runs = len(config_lst)\n",
        "\n",
        "answers = [[] for _ in range(n_runs)]\n",
        "sources = [[] for _ in range(n_runs)]\n",
        "source_docs = [[] for _ in range(n_runs)]\n",
        "\n",
        "time_scores = [[] for _ in range(n_runs)]\n",
        "faithfulness_scores = [[] for _ in range(n_runs)]\n",
        "relevancy_scores = [[] for _ in range(n_runs)]\n",
        "correctness_scores = [[] for _ in range(n_runs)]\n",
        "\n",
        "faithfulness_false = [[] for _ in range(n_runs)]\n",
        "relevancy_false = [[] for _ in range(n_runs)]\n",
        "correctness_false = [[] for _ in range(n_runs)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_317535/3523915045.py:36: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk size:  128\n",
            "Schema does not exist, rebuilding and then storing in db\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "Splitting: 14\n",
            "Splitting: 15\n",
            "Splitting: 16\n",
            "Splitting: 17\n",
            "Splitting: 18\n",
            "Splitting: 19\n",
            "Splitting: 20\n",
            "Splitting: 21\n",
            "Splitting: 22\n",
            "Splitting: 23\n",
            "Splitting: 24\n",
            "Splitting: 25\n",
            "Splitting: 26\n",
            "Splitting: 27\n",
            "Splitting: 28\n",
            "Splitting: 29\n",
            "Splitting: 30\n",
            "Splitting: 31\n",
            "Splitting: 32\n",
            "Splitting: 33\n",
            "Splitting: 34\n",
            "Splitting: 35\n",
            "Splitting: 36\n",
            "Splitting: 37\n",
            "Splitting: 38\n",
            "Splitting: 39\n",
            "Splitting: 40\n",
            "Splitting: 41\n",
            "Splitting: 42\n",
            "Splitting: 43\n",
            "Splitting: 44\n",
            "Splitting: 45\n",
            "Splitting: 46\n",
            "Splitting: 47\n",
            "Splitting: 48\n",
            "Splitting: 49\n",
            "Splitting: 50\n",
            "Splitting: 51\n",
            "Splitting: 52\n",
            "Splitting: 53\n",
            "Splitting: 54\n",
            "Splitting: 55\n",
            "Splitting: 56\n",
            "Splitting: 57\n",
            "Splitting: 58\n",
            "Splitting: 59\n",
            "Splitting: 60\n",
            "Splitting: 61\n",
            "Splitting: 62\n",
            "Splitting: 63\n",
            "Splitting: 64\n",
            "Splitting: 65\n",
            "Splitting: 66\n",
            "Splitting: 67\n",
            "Splitting: 68\n",
            "Splitting: 69\n",
            "Splitting: 70\n",
            "Splitting: 71\n",
            "Splitting: 72\n",
            "Splitting: 73\n",
            "Splitting: 74\n",
            "Splitting: 75\n",
            "Splitting: 76\n",
            "Splitting: 77\n",
            "Splitting: 78\n",
            "Splitting: 79\n",
            "Splitting: 80\n",
            "Splitting: 81\n",
            "Splitting: 82\n",
            "Splitting: 83\n",
            "Splitting: 84\n",
            "Splitting: 85\n",
            "Splitting: 86\n",
            "Splitting: 87\n",
            "Splitting: 88\n",
            "Splitting: 89\n",
            "Splitting: 90\n",
            "Splitting: 91\n",
            "Splitting: 92\n",
            "Splitting: 93\n",
            "Splitting: 94\n",
            "Splitting: 95\n",
            "Splitting: 96\n",
            "Splitting: 97\n",
            "Splitting: 98\n",
            "Splitting: 99\n",
            "Splitting: 100\n",
            "Splitting: 101\n",
            "Splitting: 102\n",
            "Splitting: 103\n",
            "Splitting: 104\n",
            "Splitting: 105\n",
            "Splitting: 106\n",
            "Splitting: 107\n",
            "Splitting: 108\n",
            "Splitting: 109\n",
            "Splitting: 110\n",
            "Splitting: 111\n",
            "Splitting: 112\n",
            "Splitting: 113\n",
            "Splitting: 114\n",
            "Splitting: 115\n",
            "Splitting: 116\n",
            "Splitting: 117\n",
            "Splitting: 118\n",
            "Splitting: 119\n",
            "Splitting: 120\n",
            "Splitting: 121\n",
            "Splitting: 122\n",
            "Splitting: 123\n",
            "Splitting: 124\n",
            "Splitting: 125\n",
            "Splitting: 126\n",
            "Splitting: 127\n",
            "Splitting: 128\n",
            "Splitting: 129\n",
            "Splitting: 130\n",
            "Splitting: 131\n",
            "Splitting: 132\n",
            "Splitting: 133\n",
            "Splitting: 134\n",
            "Splitting: 135\n",
            "Splitting: 136\n",
            "Splitting: 137\n",
            "Splitting: 138\n",
            "Splitting: 139\n",
            "Splitting: 140\n",
            "Splitting: 141\n",
            "Splitting: 142\n",
            "Splitting: 143\n",
            "Splitting: 144\n",
            "Splitting: 145\n",
            "Splitting: 146\n",
            "Splitting: 147\n",
            "Splitting: 148\n",
            "Splitting: 149\n",
            "Splitting: 150\n",
            "Splitting: 151\n",
            "Splitting: 152\n",
            "Splitting: 153\n",
            "Splitting: 154\n",
            "Splitting: 155\n",
            "Splitting: 156\n",
            "Splitting: 157\n",
            "Splitting: 158\n",
            "Splitting: 159\n",
            "Splitting: 160\n",
            "Splitting: 161\n",
            "Splitting: 162\n",
            "Splitting: 163\n",
            "Splitting: 164\n",
            "Splitting: 165\n",
            "Splitting: 166\n",
            "Splitting: 167\n",
            "Splitting: 168\n",
            "Splitting: 169\n",
            "Splitting: 170\n",
            "Splitting: 171\n",
            "Splitting: 172\n",
            "Splitting: 173\n",
            "Splitting: 174\n",
            "Splitting: 175\n",
            "Splitting: 176\n",
            "Splitting: 177\n",
            "Splitting: 178\n",
            "Splitting: 179\n",
            "Splitting: 180\n",
            "Splitting: 181\n",
            "Splitting: 182\n",
            "Splitting: 183\n",
            "Splitting: 184\n",
            "Splitting: 185\n",
            "Splitting: 186\n",
            "Splitting: 187\n",
            "Splitting: 188\n",
            "Splitting: 189\n",
            "Splitting: 190\n",
            "Splitting: 191\n",
            "Splitting: 192\n",
            "Splitting: 193\n",
            "Splitting: 194\n",
            "Splitting: 195\n",
            "Splitting: 196\n",
            "Splitting: 197\n",
            "Splitting: 198\n",
            "Splitting: 199\n",
            "Splitting: 200\n",
            "Splitting: 201\n",
            "Splitting: 202\n",
            "Splitting: 203\n",
            "Splitting: 204\n",
            "Splitting: 205\n",
            "Splitting: 206\n",
            "Splitting: 207\n",
            "Splitting: 208\n",
            "Splitting: 209\n",
            "Splitting: 210\n",
            "Splitting: 211\n",
            "Splitting: 212\n",
            "Splitting: 213\n",
            "Splitting: 214\n",
            "Splitting: 215\n",
            "Splitting: 216\n",
            "Splitting: 217\n",
            "Splitting: 218\n",
            "Splitting: 219\n",
            "Splitting: 220\n",
            "Splitting: 221\n",
            "Splitting: 222\n",
            "Splitting: 223\n",
            "Splitting: 224\n",
            "Splitting: 225\n",
            "Splitting: 226\n",
            "Splitting: 227\n",
            "Splitting: 228\n",
            "Splitting: 229\n",
            "Splitting: 230\n",
            "Splitting: 231\n",
            "Splitting: 232\n",
            "Splitting: 233\n",
            "Splitting: 234\n",
            "Splitting: 235\n",
            "Splitting: 236\n",
            "Splitting: 237\n",
            "Splitting: 238\n",
            "Splitting: 239\n",
            "Splitting: 240\n",
            "Splitting: 241\n",
            "Splitting: 242\n",
            "Splitting: 243\n",
            "Splitting: 244\n",
            "Splitting: 245\n",
            "Splitting: 246\n",
            "Splitting: 247\n",
            "Splitting: 248\n",
            "Splitting: 249\n",
            "Splitting: 250\n",
            "Splitting: 251\n",
            "Splitting: 252\n",
            "Splitting: 253\n",
            "Splitting: 254\n",
            "Splitting: 255\n",
            "Splitting: 256\n",
            "Splitting: 257\n",
            "Splitting: 258\n",
            "Splitting: 259\n",
            "Splitting: 260\n",
            "Splitting: 261\n",
            "Splitting: 262\n",
            "Splitting: 263\n",
            "Splitting: 264\n",
            "Splitting: 265\n",
            "Splitting: 266\n",
            "Splitting: 267\n",
            "Splitting: 268\n",
            "Splitting: 269\n",
            "Splitting: 270\n",
            "Splitting: 271\n",
            "Splitting: 272\n",
            "Splitting: 273\n",
            "Splitting: 274\n",
            "Splitting: 275\n",
            "Splitting: 276\n",
            "Splitting: 277\n",
            "Splitting: 278\n",
            "Splitting: 279\n",
            "Splitting: 280\n",
            "Splitting: 281\n",
            "Splitting: 282\n",
            "Splitting: 283\n",
            "Splitting: 284\n",
            "Splitting: 285\n",
            "Splitting: 286\n",
            "Splitting: 287\n",
            "Splitting: 288\n",
            "Splitting: 289\n",
            "Splitting: 290\n",
            "Splitting: 291\n",
            "Splitting: 292\n",
            "Splitting: 293\n",
            "Splitting: 294\n",
            "Splitting: 295\n",
            "Splitting: 296\n",
            "Splitting: 297\n",
            "Splitting: 298\n",
            "Splitting: 299\n",
            "Splitting: 300\n",
            "Splitting: 301\n",
            "Splitting: 302\n",
            "Splitting: 303\n",
            "Splitting: 304\n",
            "Splitting: 305\n",
            "Splitting: 306\n",
            "Splitting: 307\n",
            "Splitting: 308\n",
            "Splitting: 309\n",
            "Splitting: 310\n",
            "Splitting: 311\n",
            "Splitting: 312\n",
            "Splitting: 313\n",
            "Splitting: 314\n",
            "Splitting: 315\n",
            "Splitting: 316\n",
            "Splitting: 317\n",
            "Splitting: 318\n",
            "num nodes: 33736\n",
            "Chunk size:  128\n",
            "Schema exists already, load cached from database\n",
            "Chunk size:  128\n",
            "Schema exists already, load cached from database\n"
          ]
        }
      ],
      "source": [
        "vector_indices = []\n",
        "\n",
        "def build_all_indices():\n",
        "    for chunk_size, _, _, _, _ in config_lst:\n",
        "        vector_indices.append(build_index(eval_documents, chunk_size))\n",
        "\n",
        "build_all_indices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUncIIxR6gVz"
      },
      "source": [
        "## **Response Evaluation For A Chunk Size**\n",
        "\n",
        "We evaluate each chunk_size based on 3 metrics.\n",
        "\n",
        "1. Average Response Time.\n",
        "2. Average Faithfulness.\n",
        "3. Average Relevancy.\n",
        "\n",
        "Here's a function, `evaluate_config`, that does just that which has:\n",
        "\n",
        "1. VectorIndex Creation.\n",
        "2. Building the Query Engine**.**\n",
        "3. Metrics Calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper methods\n",
        "\n",
        "def build_query_engine(vector_index, similarity_top_k, is_hybrid):\n",
        "    print(\"== BUILDING QUERY ENGINE ==\")\n",
        "    if is_hybrid:\n",
        "        return vector_index.as_query_engine(\n",
        "            similarity_top_k=similarity_top_k, embed_model=Settings.embed_model,\n",
        "            vector_store_query_mode=\"hybrid\", alpha=0.0  # BM25\n",
        "        )\n",
        "    # -- VEC ONLY --\n",
        "    return vector_index.as_query_engine(\n",
        "        similarity_top_k=similarity_top_k, embed_model=Settings.embed_model,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "dEC2Lr0z6p1N"
      },
      "outputs": [],
      "source": [
        "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
        "# We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n",
        "from llama_index.core.base.response.schema import Response\n",
        "\n",
        "\n",
        "def evaluate_config(chunk_size, similarity_top_k, llm, run_i, eval_qa=eval_qa, label=\"default\", is_hybrid=True, is_rag=True):\n",
        "    \"\"\"\n",
        "    Evaluate the average response time, faithfulness, and relevancy of responses for a given chunk size.\n",
        "    \"\"\"\n",
        "\n",
        "    Settings.llm = llm\n",
        "\n",
        "    vector_index = vector_indices[run_i]\n",
        "\n",
        "    # Build query engine\n",
        "    query_engine = build_query_engine(\n",
        "        vector_index, similarity_top_k, is_hybrid)\n",
        "\n",
        "    # Iterate over each question in eval_questions to compute metrics.\n",
        "    # print(\"=========== QA pairs\")\n",
        "    for q, a in eval_qa:\n",
        "        print(\"--Q: \", q)\n",
        "        start_time = time.time()\n",
        "\n",
        "        if is_rag:\n",
        "            response_vector = query_engine.query(q)\n",
        "            print(\"--A: \", str(response_vector))\n",
        "\n",
        "            # get unique list of file paths of source docs\n",
        "            if response_vector.metadata is not None:\n",
        "                print(\"--sources: \")\n",
        "                raw_file_paths = list(set(value['file_path']\n",
        "                                          for value in response_vector.metadata.values()))\n",
        "                source_file_paths = list(\n",
        "                    map(get_filepath_substring, raw_file_paths))\n",
        "                print(source_file_paths)\n",
        "                sources[run_i].append(source_file_paths)\n",
        "                print(\"----------\")\n",
        "            else:\n",
        "                source_file_paths = []\n",
        "                sources[run_i].append(source_file_paths)\n",
        "                print(\"----------\")\n",
        "            source_docs[run_i].append(response_vector.get_formatted_sources)\n",
        "\n",
        "            print(f\"---docs: {len(response_vector.get_formatted_sources())}\")\n",
        "            # print(str(response_vector.get_formatted_sources(length=1000)))\n",
        "            # print(\"----------\")\n",
        "        else:\n",
        "            # this seems to be the case with RAG systems\n",
        "            q += \" Make the answer one sentence long.\"\n",
        "            response_vector = None\n",
        "            response = str(OpenAI().complete(q))\n",
        "            print(\"--A: \", str(response))\n",
        "            response_vector = Response(response)\n",
        "            sources[run_i].append([])\n",
        "\n",
        "        answers[run_i].append(response_vector)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        faithfulness_result = faithfulness_gpt.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing if is_rag else False\n",
        "\n",
        "        relevancy_result = relevancy_gpt.evaluate_response(\n",
        "            query=q, response=response_vector\n",
        "        ).passing if is_rag else False\n",
        "\n",
        "        if not faithfulness_result:\n",
        "            faithfulness_false[run_i].append((q, str(response_vector)))\n",
        "        if not relevancy_result:\n",
        "            relevancy_false[run_i].append((q, str(response_vector)))\n",
        "\n",
        "        try:\n",
        "            correctness_result = correctness_gpt.evaluate(\n",
        "                query=q,\n",
        "                response=str(response_vector),\n",
        "                reference=a,\n",
        "            )\n",
        "            correctness_scores[run_i].append(\n",
        "                (correctness_result.score, correctness_result.feedback))\n",
        "            if correctness_result.score < 2.0:\n",
        "                correctness_false[run_i].append(\n",
        "                    (q, str(response_vector), (correctness_result.score, correctness_result.feedback)))\n",
        "        except:\n",
        "            print(\"Exception occured\")\n",
        "            correctness_scores[run_i].append(None)\n",
        "\n",
        "        time_scores[run_i].append(elapsed_time)\n",
        "        faithfulness_scores[run_i].append(faithfulness_result)\n",
        "        relevancy_scores[run_i].append(relevancy_result)\n",
        "\n",
        "        print(\n",
        "            f\"t={elapsed_time}, f={faithfulness_result}, r={relevancy_result}, c={correctness_result.score}\\n-------\")\n",
        "\n",
        "    print(\"===========\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8DQvTP96s48"
      },
      "source": [
        "## **Testing Across Different Chunk Sizes**\n",
        "\n",
        "We'll evaluate a range of chunk sizes to identify which offers the most promising metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "jlKICwXH6Tib"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== BUILDING QUERY ENGINE ==\n",
            "--Q:  Why use NoSQL instead of SQL?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.11/asyncio/selector_events.py:864: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=77 read=idle write=<idle, bufsize=0>>\n",
            "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--A:  NoSQL databases are often chosen over SQL databases when dealing with unstructured or semi-structured data, as they offer more flexibility in handling varying data types and structures. Additionally, NoSQL databases can scale horizontally more easily, making them suitable for applications with high scalability requirements.\n",
            "--sources: \n",
            "['20-application-containerization/5-database-migrations.mdx', '14-user-management/6-logout.mdx', 'changelog.mdx', '21-working-with-databases/3-user-specific-data.mdx', '24-introduction-to-svelte/6-svelte-in-a-container.mdx', '36-other-frameworks-and-languages/3-python-and-fastapi.mdx', '34-web-security-basics/4-injection-flaws.mdx']\n",
            "----------\n",
            "---docs: 1270\n",
            "t=6.583185911178589, f=False, r=False, c=5.0\n",
            "-------\n",
            "--Q:  What is the difference between authentication and authorization?\n",
            "--A:  Authentication is the process of verifying a user's identity, typically by confirming credentials like passwords or biometric data. On the other hand, authorization involves determining what actions a user is allowed to perform after they have been authenticated. In simpler terms, authentication confirms who you are, while authorization determines what you can do.\n",
            "--sources: \n",
            "['13-authentication-and-authorization/2-authorization.mdx', '31-client-side-authentication/2-token-based-authentication.mdx', '31-client-side-authentication/3-json-web-tokens.mdx', '13-authentication-and-authorization/1-authentication.mdx']\n",
            "----------\n",
            "---docs: 1270\n",
            "Exception occured\n",
            "t=6.807139158248901, f=True, r=False, c=5.0\n",
            "-------\n",
            "--Q:  What is a UUID?\n",
            "--A:  A UUID is a universally unique identifier.\n",
            "--sources: \n",
            "['20-application-containerization/2-dockerfile-and-docker-compose.mdx', '9-forms-and-data/4-post-redirect-get.mdx', '40-javascript-primer/8-objects.mdx', '22-application-programming-interfaces/1-apis-and-json.mdx', '11-data-validation/2-introduction-to-zod.mdx', '31-client-side-authentication/1-api-keys.mdx', '28-cascading-style-sheets/7-style-libraries.mdx', '2-internet-and-http/1-introduction-to-the-internet.mdx']\n",
            "----------\n",
            "---docs: 1270\n",
            "t=5.458613872528076, f=False, r=False, c=4.0\n",
            "-------\n",
            "===========\n",
            "== BUILDING QUERY ENGINE ==\n",
            "--Q:  Why use NoSQL instead of SQL?\n",
            "--A:  NoSQL databases are often chosen over SQL databases due to concerns about scalability. NoSQL databases are designed to scale horizontally and are schemaless, allowing for more flexibility in handling complex data models. Additionally, NoSQL databases like MongoDB and Redis have evolved to support a wider range of functionalities, bridging the gap between traditional SQL and NoSQL approaches to form a unified NewSQL concept.\n",
            "--sources: \n",
            "['19-evolution-of-web-development/3-rest-client-side-frameworks-and-containerization.mdx', '16-user-specific-data/1-overview.mdx', '21-working-with-databases/3-user-specific-data.mdx', '6-data-on-server/2-key-value-stores.mdx']\n",
            "----------\n",
            "---docs: 1270\n",
            "t=2.418361186981201, f=True, r=True, c=4.5\n",
            "-------\n",
            "--Q:  What is the difference between authentication and authorization?\n",
            "--A:  Authentication is the process of identifying a user, typically by verifying their identity through various means like passwords, biometrics, or security tokens. On the other hand, authorization involves determining what actions a user is allowed to perform and what resources they can access after they have been successfully authenticated. In essence, authentication confirms who the user is, while authorization controls what the user can do within a system or application.\n",
            "--sources: \n",
            "['13-authentication-and-authorization/2-authorization.mdx', '13-authentication-and-authorization/index.mdx', '13-authentication-and-authorization/1-authentication.mdx']\n",
            "----------\n",
            "---docs: 1245\n",
            "t=2.8682100772857666, f=True, r=False, c=4.5\n",
            "-------\n",
            "--Q:  What is a UUID?\n",
            "--A:  A UUID is a 128-bit number that is designed to be unique without requiring central coordination. It serves as a standardized way of identifying resources and is commonly used for generating universally unique identifiers.\n",
            "--sources: \n",
            "['2-internet-and-http/1-introduction-to-the-internet.mdx', '12-cookies-and-sessions/5-sessions.mdx', '10-the-crud/3-create.mdx', '22-application-programming-interfaces/5-api-comprehension.mdx']\n",
            "----------\n",
            "---docs: 1270\n",
            "t=1.7668039798736572, f=True, r=True, c=4.5\n",
            "-------\n",
            "===========\n",
            "== BUILDING QUERY ENGINE ==\n",
            "--Q:  Why use NoSQL instead of SQL?\n",
            "--A:  NoSQL is better for handling unstructured or rapidly changing data, while SQL is better for structured data and complex queries.\n",
            "t=0.8905379772186279, f=False, r=False, c=3.5\n",
            "-------\n",
            "--Q:  What is the difference between authentication and authorization?\n",
            "--A:  Authentication is the process of verifying a user's identity, while authorization is the process of determining what actions a user is allowed to perform.\n",
            "t=1.2834794521331787, f=False, r=False, c=4.5\n",
            "-------\n",
            "--Q:  What is a UUID?\n",
            "--A:  A UUID (Universally Unique Identifier) is a 128-bit identifier used to uniquely identify information in computer systems.\n",
            "Exception occured\n",
            "t=2.3890392780303955, f=False, r=False, c=4.5\n",
            "-------\n",
            "===========\n"
          ]
        }
      ],
      "source": [
        "from statistics import mean\n",
        "\n",
        "# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n",
        "for run_i, (chunk_size, similarity_top_k, is_hybrid, llm, is_rag) in enumerate(config_lst):\n",
        "    evaluate_config(chunk_size, similarity_top_k, llm, run_i, eval_qa=eval_qa, is_hybrid=is_hybrid, is_rag=is_rag)\n",
        "\n",
        "evaluation_model_name = llm_evaluate.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= STATS ============\n",
            "n_questions: 3\n",
            "evaluation model: gpt-3.5-turbo\n",
            "============= MODELS ===========\n",
            "(hyb-gpt-3.5-turbo-128*8-rag) - avg time: 6.28s, avg faithf: 0.33, avg relev: 0.00 avg correct: 4.50\n",
            "(vec-gpt-3.5-turbo-128*8-rag) - avg time: 2.35s, avg faithf: 1.00, avg relev: 0.67 avg correct: 4.50\n",
            "(hyb-gpt-3.5-turbo-128*8) - avg time: 1.52s, avg faithf: 0.00, avg relev: 0.00 avg correct: 4.00\n",
            "============= QA ============\n",
            "========================================================\n",
            "---Q evaluated by gpt-3.5-turbo: Why use NoSQL instead of SQL? --- ref answer: They are designed to scale horizontally.\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-128*8-rag): NoSQL databases are often chosen over SQL databases when dealing with unstructured or semi-structured data, as they offer more flexibility in handling varying data types and structures. Additionally, NoSQL databases can scale horizontally more easily, making them suitable for applications with high scalability requirements.\n",
            "t=6.583185911178589s f=False r=False c=(5.0, 'The generated answer provides a comprehensive explanation of the reasons for choosing NoSQL over SQL, including the flexibility in handling different data types and structures, as well as the ability to scale horizontally. The answer directly addresses the user query and is both relevant and correct.')\n",
            "sources: ['20-application-containerization/5-database-migrations.mdx', '14-user-management/6-logout.mdx', 'changelog.mdx', '21-working-with-databases/3-user-specific-data.mdx', '24-introduction-to-svelte/6-svelte-in-a-container.mdx', '36-other-frameworks-and-languages/3-python-and-fastapi.mdx', '34-web-security-basics/4-injection-flaws.mdx']\n",
            "===\n",
            "(vec-gpt-3.5-turbo-128*8-rag): NoSQL databases are often chosen over SQL databases due to concerns about scalability. NoSQL databases are designed to scale horizontally and are schemaless, allowing for more flexibility in handling complex data models. Additionally, NoSQL databases like MongoDB and Redis have evolved to support a wider range of functionalities, bridging the gap between traditional SQL and NoSQL approaches to form a unified NewSQL concept.\n",
            "t=2.418361186981201s f=True r=True c=(4.5, 'The generated answer provides a detailed explanation of why NoSQL is often chosen over SQL, mentioning concerns about scalability, horizontal scaling, schemaless nature, flexibility in handling complex data models, and the evolution of NoSQL databases like MongoDB and Redis. It also introduces the concept of NewSQL. The answer is relevant and correct, but it could be improved by being more concise.')\n",
            "sources: ['19-evolution-of-web-development/3-rest-client-side-frameworks-and-containerization.mdx', '16-user-specific-data/1-overview.mdx', '21-working-with-databases/3-user-specific-data.mdx', '6-data-on-server/2-key-value-stores.mdx']\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-128*8): NoSQL is better for handling unstructured or rapidly changing data, while SQL is better for structured data and complex queries.\n",
            "t=0.8905379772186279s f=False r=False c=(3.5, 'The generated answer provides relevant information comparing NoSQL and SQL, but it contains a minor mistake by not directly addressing the scalability aspect mentioned in the user query.')\n",
            "sources: []\n",
            "========================================================\n",
            "---Q evaluated by gpt-3.5-turbo: What is the difference between authentication and authorization? --- ref answer: The term authentication refers to identifying a user. The term authorization refers to the process of verifying that the user has the rights to perform the actions that the user is trying to perform.\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-128*8-rag): Authentication is the process of verifying a user's identity, typically by confirming credentials like passwords or biometric data. On the other hand, authorization involves determining what actions a user is allowed to perform after they have been authenticated. In simpler terms, authentication confirms who you are, while authorization determines what you can do.\n",
            "t=6.807139158248901s f=True r=False c=None\n",
            "sources: ['13-authentication-and-authorization/2-authorization.mdx', '31-client-side-authentication/2-token-based-authentication.mdx', '31-client-side-authentication/3-json-web-tokens.mdx', '13-authentication-and-authorization/1-authentication.mdx']\n",
            "===\n",
            "(vec-gpt-3.5-turbo-128*8-rag): Authentication is the process of identifying a user, typically by verifying their identity through various means like passwords, biometrics, or security tokens. On the other hand, authorization involves determining what actions a user is allowed to perform and what resources they can access after they have been successfully authenticated. In essence, authentication confirms who the user is, while authorization controls what the user can do within a system or application.\n",
            "t=2.8682100772857666s f=True r=False c=(4.5, 'The generated answer provides a clear and accurate explanation of the difference between authentication and authorization, matching the reference answer closely. However, it is slightly more detailed and verbose than the reference answer, which is why it is not a perfect 5.')\n",
            "sources: ['13-authentication-and-authorization/2-authorization.mdx', '13-authentication-and-authorization/index.mdx', '13-authentication-and-authorization/1-authentication.mdx']\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-128*8): Authentication is the process of verifying a user's identity, while authorization is the process of determining what actions a user is allowed to perform.\n",
            "t=1.2834794521331787s f=False r=False c=(4.5, 'The generated answer provides a clear and concise differentiation between authentication and authorization, accurately explaining the difference in one sentence as requested. The answer is relevant and fully correct, with no mistakes in the explanation.')\n",
            "sources: []\n",
            "========================================================\n",
            "---Q evaluated by gpt-3.5-turbo: What is a UUID? --- ref answer: UUIDs are a common way of identifying resources. They are 128-bit numbers that are designed for being unique without central coordination (i.e. a service that would keep track of which identifier to assign next).\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-128*8-rag): A UUID is a universally unique identifier.\n",
            "t=5.458613872528076s f=False r=False c=(4.0, 'The generated answer is relevant and fully correct, providing a concise definition of a UUID that matches the user query.')\n",
            "sources: ['20-application-containerization/2-dockerfile-and-docker-compose.mdx', '9-forms-and-data/4-post-redirect-get.mdx', '40-javascript-primer/8-objects.mdx', '22-application-programming-interfaces/1-apis-and-json.mdx', '11-data-validation/2-introduction-to-zod.mdx', '31-client-side-authentication/1-api-keys.mdx', '28-cascading-style-sheets/7-style-libraries.mdx', '2-internet-and-http/1-introduction-to-the-internet.mdx']\n",
            "===\n",
            "(vec-gpt-3.5-turbo-128*8-rag): A UUID is a 128-bit number that is designed to be unique without requiring central coordination. It serves as a standardized way of identifying resources and is commonly used for generating universally unique identifiers.\n",
            "t=1.7668039798736572s f=True r=True c=(4.5, 'The generated answer provides a clear and accurate explanation of what a UUID is, matching the reference answer closely. It is relevant and correct, with only minor differences in wording.')\n",
            "sources: ['2-internet-and-http/1-introduction-to-the-internet.mdx', '12-cookies-and-sessions/5-sessions.mdx', '10-the-crud/3-create.mdx', '22-application-programming-interfaces/5-api-comprehension.mdx']\n",
            "===\n",
            "(hyb-gpt-3.5-turbo-128*8): A UUID (Universally Unique Identifier) is a 128-bit identifier used to uniquely identify information in computer systems.\n",
            "t=2.3890392780303955s f=False r=False c=None\n",
            "sources: []\n"
          ]
        }
      ],
      "source": [
        "print(\"============= STATS ============\")\n",
        "print(f\"n_questions: {len(eval_qa)}\")\n",
        "print(f\"evaluation model: {evaluation_model_name}\")\n",
        "print(\"============= MODELS ===========\")\n",
        "\n",
        "for config_i, (chunk_size, similarity_top_k, is_hybrid, llm, is_rag) in enumerate(config_lst):\n",
        "    time_avg = mean(time_scores[config_i])\n",
        "    faithfulness_avg = mean(faithfulness_scores[config_i])\n",
        "    relevancy_avg = mean(relevancy_scores[config_i])\n",
        "    correctness_without_none = [\n",
        "        x for x in correctness_scores[config_i] if x is not None]\n",
        "    correctness_avg = sum(\n",
        "        elt[0] for elt in correctness_without_none)/len(correctness_without_none)\n",
        "    print(\n",
        "        f\"({'hyb' if is_hybrid else 'vec'}-{llm.model}-{chunk_size}*{similarity_top_k}{'-rag' if is_rag else ''}) - avg time: {time_avg:.2f}s, avg faithf: {faithfulness_avg:.2f}, avg relev: {relevancy_avg:.2f} avg correct: {correctness_avg:.2f}\")\n",
        "\n",
        "\n",
        "print(\"============= QA ============\")\n",
        "\n",
        "for i, (q, a) in enumerate(eval_qa):\n",
        "    print(\"========================================================\")\n",
        "    print(\n",
        "        f\"---Q evaluated by {evaluation_model_name}: {q} --- ref answer: {a}\")\n",
        "    for config_i, (chunk_size, similarity_top_k, is_hybrid, llm, is_rag) in enumerate(config_lst):\n",
        "        print(\"===\")\n",
        "        print(\n",
        "            f\"({'hyb' if is_hybrid else 'vec'}-{llm.model}-{chunk_size}*{similarity_top_k}{'-rag' if is_rag else ''}): {answers[config_i][i]}\")\n",
        "        print(\n",
        "            f\"t={time_scores[config_i][i]}s f={faithfulness_scores[config_i][i]} r={relevancy_scores[config_i][i]} c={correctness_scores[config_i][i]}\")\n",
        "        print(\n",
        "            f\"sources: {sources[config_i][i]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
