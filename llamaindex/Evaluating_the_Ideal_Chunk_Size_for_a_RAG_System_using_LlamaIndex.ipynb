{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FqeieOC5vUB"
      },
      "source": [
        "# Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIvSXj365r2n"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "Retrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the `chunk_size`. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex `Response Evaluation` comes handy. In this blogpost, we'll guide you through the steps to determine the best `chunk size` using LlamaIndex’s `Response Evaluation` module. If you're unfamiliar with the `Response` Evaluation module, we recommend reviewing its [documentation](https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html) before proceeding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpbtWrEa53Ct"
      },
      "source": [
        "## **Why Chunk Size Matters**\n",
        "\n",
        "Choosing the right `chunk_size` is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n",
        "\n",
        "1. **Relevance and Granularity**: A small `chunk_size`, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the `similarity_top_k` setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
        "2. **Response Generation Time**: As the `chunk_size` increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
        "\n",
        "In essence, determining the optimal `chunk_size` is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use-case and dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR8jlf3358_z"
      },
      "source": [
        "## **Setup**\n",
        "\n",
        "Before embarking on the experiment, we need to ensure all requisite modules are imported:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ItNWVKRRD67j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: llama-index in /home/addo/.local/lib/python3.11/site-packages (0.10.7)\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /home/addo/.local/lib/python3.11/site-packages (0.1.3)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.10.7)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.2)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.2)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.3)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
            "  Downloading thinc-8.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (23.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.26.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/addo/.local/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.12.2)\n",
            "Requirement already satisfied: httpx in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.26.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.1.13)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.8.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.1)\n",
            "Requirement already satisfied: pandas in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (9.0.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (4.8.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (0.0.2)\n",
            "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (1.23.24)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/addo/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
            "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: platformdirs in /usr/lib/python3.11/site-packages (from setuptools->spacy) (3.11.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/lib/python3.11/site-packages (from setuptools->spacy) (3.11.1)\n",
            "Requirement already satisfied: more-itertools in /usr/lib/python3.11/site-packages (from setuptools->spacy) (10.1.0)\n",
            "Requirement already satisfied: ordered-set in /usr/lib/python3.11/site-packages (from setuptools->spacy) (4.1.0)\n",
            "Requirement already satisfied: tomli in /usr/lib/python3.11/site-packages (from setuptools->spacy) (2.0.1)\n",
            "Requirement already satisfied: validate-pyproject in /usr/lib/python3.11/site-packages (from setuptools->spacy) (0.13.post1.dev0+gb752273.d20230520)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /home/addo/.local/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n",
            "Requirement already satisfied: anyio in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (4.2.0)\n",
            "Requirement already satisfied: certifi in /usr/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/addo/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (0.14.0)\n",
            "Requirement already satisfied: joblib in /home/addo/.local/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.8.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.22 in /home/addo/.local/lib/python3.11/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (1.23.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/addo/.local/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/addo/.local/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/addo/.local/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.20.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (3.9.0)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (4.3.0)\n",
            "Requirement already satisfied: autocommand in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (2.2.2)\n",
            "Requirement already satisfied: inflect in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/addo/.local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/addo/.local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.4)\n",
            "Requirement already satisfied: fastjsonschema<=3,>=2.16.2 in /usr/lib/python3.11/site-packages (from validate-pyproject->setuptools->spacy) (2.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n",
            "Downloading spacy-3.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.9/490.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.1/920.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\n",
            "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-embeddings-openai spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y9SVm76h58de"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.core.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "import openai\n",
        "import time\n",
        "openai.api_key = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO21UssT6L8N"
      },
      "source": [
        "## **Load Data**\n",
        "\n",
        "Let’s load our document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "x6QdEBd-17OC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "# Load Data\n",
        "# reader = SimpleDirectoryReader(\"../data/web-software-development-1-0/\", recursive=True)\n",
        "reader = SimpleDirectoryReader(\"../data/web-software-development-1-0/13-working-with-databases-i/\", recursive=True)\n",
        "\n",
        "documents = reader.load_data()\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnpPtiz56TYA"
      },
      "source": [
        "## **Question Generation**\n",
        "\n",
        "To select the right `chunk_size`, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various `chunk_sizes`. The `DatasetGenerator` will help us generate questions from the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "26BgDF3L6Z0r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of documents:  14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/addo/.local/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:212: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
            "  return cls(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EVAL QUESTIONS ===\n",
            "['What is the importance of using a database in web applications?', 'What database management system will be used in this course?', 'What are the learning objectives related to working with databases?', 'Where can you find a tutorial for SQL basics if you need a refresher?', 'How can you start using PostgreSQL according to the document?', 'What is the recommended approach for taking PostgreSQL into use for development?', 'What is the purpose of the Walking skeleton in relation to PostgreSQL?', 'What are some options for running PostgreSQL locally?', 'Name two hosted services that provide PostgreSQL as a service.', 'Why does the document strongly recommend using the first option for development when starting to use PostgreSQL?', 'What are two options for starting to use PostgreSQL as mentioned in the document?', 'What are some examples of hosted services that provide PostgreSQL databases?', 'Why does the document strongly recommend using the first option for development?', 'How can you get started with ElephantSQL according to the document?', \"What attributes are included in the table created in the document's example using SQL?\", 'How can you add names to the table in ElephantSQL according to the document?', \"What SQL query can you use to select all rows from the 'names' table in ElephantSQL?\", \"What library is used in the document's example to access the database programatically?\", 'What information is grayed out in the image of the ElephantSQL details page?', \"What is the purpose of the 'id' attribute in the table created in the document's example?\", 'What library is used in the first example to access a PostgreSQL database in the provided code snippet?', 'How can you specify the database credentials when using Postgres.js in the provided code snippet?', 'What is the purpose of the `max: 2` parameter in the Postgres.js example?', 'In the second example, what library is used to access a PostgreSQL database?', 'What is the recommended alternative to Deno Postgres mentioned in the document?', 'How can you establish a connection to a PostgreSQL database using Deno Postgres in the provided code snippet?', 'What query is executed in the Deno Postgres example to retrieve data from the database?', 'What is the significance of having a database client when working with databases?', 'What is the default database client mentioned in the document for accessing a PostgreSQL database?', 'Where can you find a list of PostgreSQL clients for different operating systems according to the document?', 'What database driver is used when working with Deno and PostgreSQL in the provided document?', 'How can you create a database client using the Postgres.js driver?', 'In the example code provided, what SQL query is being executed to retrieve data from the database?', 'How does Postgres.js ensure safe query generation when constructing SQL queries?', 'What is the purpose of the `sql` function in the Postgres.js driver?', 'In the example code, how is the result data iterated over to print only the name property?', 'What SQL statement is used to insert data into a database in the provided document?', 'After inserting a new name into the database, how many names are present in the database according to the output?', 'What flag is required to be used with Deno when working with the Postgres.js driver?', 'How can you access the Postgres.js documentation for further details on tagged template literals?']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/addo/.local/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:309: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
            "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
          ]
        }
      ],
      "source": [
        "# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\n",
        "eval_documents = documents[:20]\n",
        "print(\"Amount of documents: \", len(eval_documents))\n",
        "\n",
        "# TODO do with more chapters, and try to instruct to create 1 question per chunk (or similar)\n",
        "\n",
        "data_generator = DatasetGenerator.from_documents(documents)\n",
        "eval_questions = data_generator.generate_questions_from_nodes(num = 40)\n",
        "print(\"=== EVAL QUESTIONS ===\")\n",
        "print(eval_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WwA-0N6dMO"
      },
      "source": [
        "## Setting Up Evaluators\n",
        "\n",
        "We are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, `FaithfulnessEvaluator` and `RelevancyEvaluator`, are initialised with the `service_context` .\n",
        "\n",
        "1. **Faithfulness Evaluator** - It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.\n",
        "2. **Relevancy Evaluator** - It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G2LoMRtr6fnG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_110624/2600024486.py:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n"
          ]
        }
      ],
      "source": [
        "# We will use GPT-4 for evaluating the responses\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "\n",
        "# Define service context for GPT-4 for evaluation\n",
        "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
        "\n",
        "# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n",
        "faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
        "relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Storing embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from llama_index.core.schema import IndexNode\n",
        "from llama_index.core import (\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.retrievers import RecursiveRetriever\n",
        "import os\n",
        "# from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "\n",
        "\n",
        "def build_index(docs, chunk_size, out_path: str):\n",
        "    print(\"Chunk size: \", chunk_size)\n",
        "\n",
        "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",\n",
        "                                  chunk_size=chunk_size,\n",
        "                                  )\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    nodes = []\n",
        "\n",
        "    splitter = SentenceSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_size/4)\n",
        "    for idx, doc in enumerate(docs):\n",
        "        print('Splitting: ' + str(idx))\n",
        "\n",
        "        cur_nodes = splitter.get_nodes_from_documents([doc])\n",
        "        for cur_node in cur_nodes:\n",
        "            # ID will be base + parent\n",
        "            file_path = doc.metadata[\"file_path\"]\n",
        "            new_node = IndexNode(\n",
        "                text=cur_node.text or \"None\",\n",
        "                index_id=str(file_path),\n",
        "                metadata=doc.metadata,\n",
        "                # obj=doc\n",
        "            )\n",
        "            nodes.append(new_node)\n",
        "    print(\"num nodes: \" + str(len(nodes)))\n",
        "\n",
        "    # save index to disk\n",
        "    if not os.path.exists(out_path):\n",
        "        index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
        "        index.set_index_id(\"simple_index\")\n",
        "        index.storage_context.persist(f\"./{out_path}\")\n",
        "    else:\n",
        "        # rebuild storage context\n",
        "        storage_context = StorageContext.from_defaults(\n",
        "            persist_dir=f\"./{out_path}\"\n",
        "        )\n",
        "        # load index\n",
        "        index = load_index_from_storage(\n",
        "            storage_context, index_id=\"simple_index\", embed_model=embed_model\n",
        "        )\n",
        "\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_sizes = [128, 256, 512, 1024, 2048]\n",
        "similarities_top_k = [8, 6, 4, 2, 1]\n",
        "\n",
        "# for chunk_size in chunk_sizes:\n",
        "#     llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "#     service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n",
        "#     vector_index = VectorStoreIndex.from_documents(\n",
        "#         eval_documents, service_context=service_context\n",
        "#     )\n",
        "    \n",
        "#     vector_index.storage_context.persist(persist_dir=f\"vector_stores/openai_{chunk_size}-wsd\")\n",
        "#     vector_indices.append(vector_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUncIIxR6gVz"
      },
      "source": [
        "## **Response Evaluation For A Chunk Size**\n",
        "\n",
        "We evaluate each chunk_size based on 3 metrics.\n",
        "\n",
        "1. Average Response Time.\n",
        "2. Average Faithfulness.\n",
        "3. Average Relevancy.\n",
        "\n",
        "Here's a function, `evaluate_response_time_and_accuracy`, that does just that which has:\n",
        "\n",
        "1. VectorIndex Creation.\n",
        "2. Building the Query Engine**.**\n",
        "3. Metrics Calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dEC2Lr0z6p1N"
      },
      "outputs": [],
      "source": [
        "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
        "# We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n",
        "def evaluate_response_time_and_accuracy(chunk_size, similarity_top_k, eval_questions=eval_questions):\n",
        "    \"\"\"\n",
        "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
        "\n",
        "    Parameters:\n",
        "    chunk_size (int): The size of data chunks being processed.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    total_response_time = 0\n",
        "    total_faithfulness = 0\n",
        "    total_relevancy = 0\n",
        "\n",
        "    # Load vector_index\n",
        "    vector_index = build_index(eval_documents, chunk_size, f\"vector_stores/openai_{chunk_size}-wsd\")\n",
        "\n",
        "    # build query engine\n",
        "    # By default, similarity_top_k is set to 2. To experiment with different values, pass it as an argument to as_query_engine()\n",
        "    query_engine = vector_index.as_query_engine(similarity_top_k=similarity_top_k, embed_model=Settings.embed_model)\n",
        "    num_questions = len(eval_questions)\n",
        "\n",
        "    # Iterate over each question in eval_questions to compute metrics.\n",
        "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
        "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
        "    print(\"=========== QA pairs\")\n",
        "    for question in eval_questions:\n",
        "        print(\"--Q\\n\", question, \"\\n--\")\n",
        "        start_time = time.time()\n",
        "        response_vector = query_engine.query(question)\n",
        "        print(\"--A\\n\", response_vector, \"\\n--\")\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
        "            query=question, response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        total_response_time += elapsed_time\n",
        "        total_faithfulness += faithfulness_result\n",
        "        total_relevancy += relevancy_result\n",
        "\n",
        "    print(\"===========\")\n",
        "    average_response_time = total_response_time / num_questions\n",
        "    average_faithfulness = total_faithfulness / num_questions\n",
        "    average_relevancy = total_relevancy / num_questions\n",
        "\n",
        "    return average_response_time, average_faithfulness, average_relevancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from llama_index.core.base.response.schema import Response\n",
        "\n",
        "def evaluate_response_time_and_accuracy_without_rag(eval_questions=eval_questions):\n",
        "    \"\"\"\n",
        "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
        "\n",
        "    Parameters:\n",
        "    chunk_size (int): The size of data chunks being processed.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    total_response_time = 0\n",
        "    total_faithfulness = 0\n",
        "    total_relevancy = 0\n",
        "\n",
        "    # By default, similarity_top_k is set to 2. To experiment with different values, pass it as an argument to as_query_engine()\n",
        "    num_questions = len(eval_questions)\n",
        "\n",
        "    index = VectorStoreIndex(nodes=[])\n",
        "    query_engine = index.as_query_engine()\n",
        "\n",
        "    # Iterate over each question in eval_questions to compute metrics.\n",
        "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
        "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
        "    print(\"=========== QA pairs\")\n",
        "    for question in eval_questions:\n",
        "        print(\"--Q\\n\", question, \"\\n--\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        response_vector = query_engine.query(question)\n",
        "\n",
        "        print(\"--A\\n\", response_vector, \"\\n--\")\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
        "            query=question, response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        total_response_time += elapsed_time\n",
        "        total_faithfulness += faithfulness_result\n",
        "        total_relevancy += relevancy_result\n",
        "\n",
        "        # TODO: both response and retrieval evaluation\n",
        "\n",
        "    print(\"===========\")\n",
        "    average_response_time = total_response_time / num_questions\n",
        "    average_faithfulness = total_faithfulness / num_questions\n",
        "    average_relevancy = total_relevancy / num_questions\n",
        "\n",
        "    return average_response_time, average_faithfulness, average_relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8DQvTP96s48"
      },
      "source": [
        "## **Testing Across Different Chunk Sizes**\n",
        "\n",
        "We'll evaluate a range of chunk sizes to identify which offers the most promising metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "jlKICwXH6Tib"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk size:  128\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "num nodes: 1330\n",
            "Chunk size 128 - Similarities_top_k [8, 6, 4, 2, 1] - Average Response time: 2.23s, Average Faithfulness: 1.00, Average Relevancy: 0.97\n",
            "Chunk size:  256\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "num nodes: 574\n",
            "Chunk size 256 - Similarities_top_k [8, 6, 4, 2, 1] - Average Response time: 2.10s, Average Faithfulness: 0.97, Average Relevancy: 0.93\n",
            "Chunk size:  512\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "num nodes: 267\n",
            "Chunk size 512 - Similarities_top_k [8, 6, 4, 2, 1] - Average Response time: 1.88s, Average Faithfulness: 0.97, Average Relevancy: 0.97\n",
            "Chunk size:  1024\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "num nodes: 133\n",
            "Chunk size 1024 - Similarities_top_k [8, 6, 4, 2, 1] - Average Response time: 1.80s, Average Faithfulness: 0.90, Average Relevancy: 0.88\n",
            "Chunk size:  2048\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "num nodes: 70\n",
            "Chunk size 2048 - Similarities_top_k [8, 6, 4, 2, 1] - Average Response time: 1.72s, Average Faithfulness: 0.80, Average Relevancy: 0.85\n"
          ]
        }
      ],
      "source": [
        "# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n",
        "for chunk_size, similarity_top_k in zip(chunk_sizes, similarities_top_k):\n",
        "    avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, similarity_top_k, eval_questions=eval_questions)\n",
        "    print(f\"Chunk size {chunk_size} - Similarities_top_k {similarity_top_k} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========== QA pairs\n",
            "--Q\n",
            " What is the importance of using a database in web applications? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " What database management system will be used in this course? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " What are the learning objectives related to working with databases? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " Where can you find a tutorial for SQL basics if you need a refresher? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " How can you start using PostgreSQL according to the document? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " What is the recommended approach for taking PostgreSQL into use for development? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " What is the purpose of the Walking skeleton in relation to PostgreSQL? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " What are some options for running PostgreSQL locally? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " Name two hosted services that provide PostgreSQL as a service. \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " Why does the document strongly recommend using the first option for development when starting to use PostgreSQL? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " What are two options for starting to use PostgreSQL as mentioned in the document? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " What are some examples of hosted services that provide PostgreSQL databases? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " Why does the document strongly recommend using the first option for development? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " How can you get started with ElephantSQL according to the document? \n",
            "--\n",
            "--A\n",
            " Empty Response \n",
            "--\n",
            "--Q\n",
            " What attributes are included in the table created in the document's example using SQL? \n",
            "--\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate_response_time_and_accuracy_without_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_questions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_questions\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[52], line 32\u001b[0m, in \u001b[0;36mevaluate_response_time_and_accuracy_without_rag\u001b[0;34m(eval_questions)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--Q\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 32\u001b[0m response_vector \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--A\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, response_vector, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/base/base_query_engine.py:40\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(str_or_query_bundle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m     39\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 40\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py:186\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Answer a query.\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    184\u001b[0m     CBEventType\u001b[39m.\u001b[39mQUERY, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str}\n\u001b[1;32m    185\u001b[0m ) \u001b[39mas\u001b[39;00m query_event:\n\u001b[0;32m--> 186\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve(query_bundle)\n\u001b[1;32m    187\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_synthesizer\u001b[39m.\u001b[39msynthesize(\n\u001b[1;32m    188\u001b[0m         query\u001b[39m=\u001b[39mquery_bundle,\n\u001b[1;32m    189\u001b[0m         nodes\u001b[39m=\u001b[39mnodes,\n\u001b[1;32m    190\u001b[0m     )\n\u001b[1;32m    192\u001b[0m     query_event\u001b[39m.\u001b[39mon_end(payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mRESPONSE: response})\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/query_engine/retriever_query_engine.py:142\u001b[0m, in \u001b[0;36mRetrieverQueryEngine.retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mretrieve\u001b[39m(\u001b[39mself\u001b[39m, query_bundle: QueryBundle) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[NodeWithScore]:\n\u001b[0;32m--> 142\u001b[0m     nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retriever\u001b[39m.\u001b[39;49mretrieve(query_bundle)\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_node_postprocessors(nodes, query_bundle\u001b[39m=\u001b[39mquery_bundle)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/base/base_retriever.py:229\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mas_trace(\u001b[39m\"\u001b[39m\u001b[39mquery\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    225\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    226\u001b[0m         CBEventType\u001b[39m.\u001b[39mRETRIEVE,\n\u001b[1;32m    227\u001b[0m         payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mQUERY_STR: query_bundle\u001b[39m.\u001b[39mquery_str},\n\u001b[1;32m    228\u001b[0m     ) \u001b[39mas\u001b[39;00m retrieve_event:\n\u001b[0;32m--> 229\u001b[0m         nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retrieve(query_bundle)\n\u001b[1;32m    230\u001b[0m         nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_recursive_retrieval(query_bundle, nodes)\n\u001b[1;32m    231\u001b[0m         retrieve_event\u001b[39m.\u001b[39mon_end(\n\u001b[1;32m    232\u001b[0m             payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mNODES: nodes},\n\u001b[1;32m    233\u001b[0m         )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/indices/vector_store/retrievers/retriever.py:90\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39mis_embedding_query:\n\u001b[1;32m     88\u001b[0m     \u001b[39mif\u001b[39;00m query_bundle\u001b[39m.\u001b[39membedding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(query_bundle\u001b[39m.\u001b[39membedding_strs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     89\u001b[0m         query_bundle\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m (\n\u001b[0;32m---> 90\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embed_model\u001b[39m.\u001b[39;49mget_agg_embedding_from_queries(\n\u001b[1;32m     91\u001b[0m                 query_bundle\u001b[39m.\u001b[39;49membedding_strs\n\u001b[1;32m     92\u001b[0m             )\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_nodes_with_embeddings(query_bundle)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:141\u001b[0m, in \u001b[0;36mBaseEmbedding.get_agg_embedding_from_queries\u001b[0;34m(self, queries, agg_fn)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     queries: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    138\u001b[0m     agg_fn: Optional[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, Embedding]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    139\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Embedding:\n\u001b[1;32m    140\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     query_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_query_embedding(query) \u001b[39mfor\u001b[39;49;00m query \u001b[39min\u001b[39;49;00m queries]\n\u001b[1;32m    142\u001b[0m     agg_fn \u001b[39m=\u001b[39m agg_fn \u001b[39mor\u001b[39;00m mean_agg\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:141\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_agg_embedding_from_queries\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     queries: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    138\u001b[0m     agg_fn: Optional[Callable[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, Embedding]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    139\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Embedding:\n\u001b[1;32m    140\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get aggregated embedding from multiple queries.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     query_embeddings \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_query_embedding(query) \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries]\n\u001b[1;32m    142\u001b[0m     agg_fn \u001b[39m=\u001b[39m agg_fn \u001b[39mor\u001b[39;00m mean_agg\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m agg_fn(query_embeddings)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:110\u001b[0m, in \u001b[0;36mBaseEmbedding.get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39mEmbed the input query.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39membeddings/huggingface_utils.py.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mevent(\n\u001b[1;32m    108\u001b[0m     CBEventType\u001b[39m.\u001b[39mEMBEDDING, payload\u001b[39m=\u001b[39m{EventPayload\u001b[39m.\u001b[39mSERIALIZED: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dict()}\n\u001b[1;32m    109\u001b[0m ) \u001b[39mas\u001b[39;00m event:\n\u001b[0;32m--> 110\u001b[0m     query_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_query_embedding(query)\n\u001b[1;32m    112\u001b[0m     event\u001b[39m.\u001b[39mon_end(\n\u001b[1;32m    113\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[1;32m    114\u001b[0m             EventPayload\u001b[39m.\u001b[39mCHUNKS: [query],\n\u001b[1;32m    115\u001b[0m             EventPayload\u001b[39m.\u001b[39mEMBEDDINGS: [query_embedding],\n\u001b[1;32m    116\u001b[0m         },\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m query_embedding\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:374\u001b[0m, in \u001b[0;36mOpenAIEmbedding._get_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get query embedding.\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_client()\n\u001b[0;32m--> 374\u001b[0m \u001b[39mreturn\u001b[39;00m get_embedding(\n\u001b[1;32m    375\u001b[0m     client,\n\u001b[1;32m    376\u001b[0m     query,\n\u001b[1;32m    377\u001b[0m     engine\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query_engine,\n\u001b[1;32m    378\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madditional_kwargs,\n\u001b[1;32m    379\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
            "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "File \u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/llama_index/embeddings/openai/base.py:137\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(client, text, engine, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get embedding.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m \u001b[39mNOTE: Copied from OpenAI's embedding utils:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 137\u001b[0m     client\u001b[39m.\u001b[39;49membeddings\u001b[39m.\u001b[39;49mcreate(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m[text], model\u001b[39m=\u001b[39;49mengine, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mdata[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39membedding\n\u001b[1;32m    138\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/resources/embeddings.py:106\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    100\u001b[0m         embedding\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfrombuffer(  \u001b[39m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m             base64\u001b[39m.\u001b[39mb64decode(data), dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         )\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m--> 106\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    107\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m/embeddings\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    108\u001b[0m     body\u001b[39m=\u001b[39;49mmaybe_transform(params, embedding_create_params\u001b[39m.\u001b[39;49mEmbeddingCreateParams),\n\u001b[1;32m    109\u001b[0m     options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    110\u001b[0m         extra_headers\u001b[39m=\u001b[39;49mextra_headers,\n\u001b[1;32m    111\u001b[0m         extra_query\u001b[39m=\u001b[39;49mextra_query,\n\u001b[1;32m    112\u001b[0m         extra_body\u001b[39m=\u001b[39;49mextra_body,\n\u001b[1;32m    113\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    114\u001b[0m         post_parser\u001b[39m=\u001b[39;49mparser,\n\u001b[1;32m    115\u001b[0m     ),\n\u001b[1;32m    116\u001b[0m     cast_to\u001b[39m=\u001b[39;49mCreateEmbeddingResponse,\n\u001b[1;32m    117\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1075\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1076\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1084\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1085\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1086\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[0;32m-> 1088\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    845\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    854\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    855\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    856\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    857\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    858\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    859\u001b[0m     )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/openai/_base_client.py:877\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    876\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 877\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(\n\u001b[1;32m    878\u001b[0m         request,\n\u001b[1;32m    879\u001b[0m         auth\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_auth,\n\u001b[1;32m    880\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_should_stream_response_body(request\u001b[39m=\u001b[39;49mrequest),\n\u001b[1;32m    881\u001b[0m     )\n\u001b[1;32m    882\u001b[0m \u001b[39mexcept\u001b[39;00m httpx\u001b[39m.\u001b[39mTimeoutException \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    883\u001b[0m     \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    907\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    908\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    909\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    910\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    911\u001b[0m )\n\u001b[1;32m    913\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 915\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    916\u001b[0m     request,\n\u001b[1;32m    917\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    918\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    919\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    920\u001b[0m )\n\u001b[1;32m    921\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    942\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    944\u001b[0m         request,\n\u001b[1;32m    945\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    946\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    947\u001b[0m     )\n\u001b[1;32m    948\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    978\u001b[0m     hook(request)\n\u001b[0;32m--> 980\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    981\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1012\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1016\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1018\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1020\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpx/_transports/default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    218\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    219\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    220\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 231\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    233\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    236\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    237\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    238\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    239\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    240\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[39mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m    252\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[39m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[39m# status so that the request becomes queued again.\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mresponse_closed\u001b[39m\u001b[39m\"\u001b[39m, logger, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreceive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    112\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    177\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    213\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
            "File \u001b[0;32m~/.local/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
            "File \u001b[0;32m/usr/lib/python3.11/ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1293\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1294\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1295\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1296\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
            "File \u001b[0;32m/usr/lib/python3.11/ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1170\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1171\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "evaluate_response_time_and_accuracy_without_rag(eval_questions=eval_questions)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
