{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FqeieOC5vUB"
      },
      "source": [
        "# Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIvSXj365r2n"
      },
      "source": [
        "# **Introduction**\n",
        "\n",
        "Retrieval-augmented generation (RAG) has introduced an innovative approach that fuses the extensive retrieval capabilities of search systems with the LLM. When implementing a RAG system, one critical parameter that governs the system’s efficiency and performance is the `chunk_size`. How does one discern the optimal chunk size for seamless retrieval? This is where LlamaIndex `Response Evaluation` comes handy. In this blogpost, we'll guide you through the steps to determine the best `chunk size` using LlamaIndex’s `Response Evaluation` module. If you're unfamiliar with the `Response` Evaluation module, we recommend reviewing its [documentation](https://docs.llamaindex.ai/en/latest/core_modules/supporting_modules/evaluation/modules.html) before proceeding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpbtWrEa53Ct"
      },
      "source": [
        "## **Why Chunk Size Matters**\n",
        "\n",
        "Choosing the right `chunk_size` is a critical decision that can influence the efficiency and accuracy of a RAG system in several ways:\n",
        "\n",
        "1. **Relevance and Granularity**: A small `chunk_size`, like 128, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the `similarity_top_k` setting is as restrictive as 2. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
        "2. **Response Generation Time**: As the `chunk_size` increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
        "\n",
        "In essence, determining the optimal `chunk_size` is about striking a balance: capturing all essential information without sacrificing speed. It's vital to undergo thorough testing with various sizes to find a configuration that suits the specific use-case and dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR8jlf3358_z"
      },
      "source": [
        "## **Setup**\n",
        "\n",
        "Before embarking on the experiment, we need to ensure all requisite modules are imported:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ItNWVKRRD67j"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: llama-index in /home/addo/.local/lib/python3.11/site-packages (0.10.7)\n",
            "Requirement already satisfied: llama-index-embeddings-openai in /home/addo/.local/lib/python3.11/site-packages (0.1.3)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.10.7)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.2)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.2)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.1)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index) (0.1.3)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
            "  Downloading thinc-8.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.9.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
            "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3.11/site-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (23.2)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /home/addo/.local/lib/python3.11/site-packages (from spacy) (1.26.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /home/addo/.local/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.9.3)\n",
            "Requirement already satisfied: dataclasses-json in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.6.4)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.12.2)\n",
            "Requirement already satisfied: httpx in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.26.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.1.13)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (3.8.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (1.6.1)\n",
            "Requirement already satisfied: pandas in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (2.1.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (9.0.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (4.8.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (0.0.2)\n",
            "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (1.23.24)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /home/addo/.local/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/addo/.local/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
            "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
            "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
            "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: platformdirs in /usr/lib/python3.11/site-packages (from setuptools->spacy) (3.11.0)\n",
            "Requirement already satisfied: jaraco.text in /usr/lib/python3.11/site-packages (from setuptools->spacy) (3.11.1)\n",
            "Requirement already satisfied: more-itertools in /usr/lib/python3.11/site-packages (from setuptools->spacy) (10.1.0)\n",
            "Requirement already satisfied: ordered-set in /usr/lib/python3.11/site-packages (from setuptools->spacy) (4.1.0)\n",
            "Requirement already satisfied: tomli in /usr/lib/python3.11/site-packages (from setuptools->spacy) (2.0.1)\n",
            "Requirement already satisfied: validate-pyproject in /usr/lib/python3.11/site-packages (from setuptools->spacy) (0.13.post1.dev0+gb752273.d20230520)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /home/addo/.local/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (2.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /home/addo/.local/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n",
            "Requirement already satisfied: anyio in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (4.2.0)\n",
            "Requirement already satisfied: certifi in /usr/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.7.22)\n",
            "Requirement already satisfied: httpcore==1.* in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.2)\n",
            "Requirement already satisfied: sniffio in /home/addo/.local/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/addo/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.0->llama-index) (0.14.0)\n",
            "Requirement already satisfied: joblib in /home/addo/.local/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.8.0)\n",
            "Requirement already satisfied: PyMuPDFb==1.23.22 in /home/addo/.local/lib/python3.11/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama-index) (1.23.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/addo/.local/lib/python3.11/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/addo/.local/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/addo/.local/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama-index) (3.20.2)\n",
            "Requirement already satisfied: jaraco.functools in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (3.9.0)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (4.3.0)\n",
            "Requirement already satisfied: autocommand in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (2.2.2)\n",
            "Requirement already satisfied: inflect in /usr/lib/python3.11/site-packages (from jaraco.text->setuptools->spacy) (7.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/addo/.local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/addo/.local/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (2023.4)\n",
            "Requirement already satisfied: fastjsonschema<=3,>=2.16.2 in /usr/lib/python3.11/site-packages (from validate-pyproject->setuptools->spacy) (2.18.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.0->llama-index) (1.16.0)\n",
            "Downloading spacy-3.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.9/490.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.1/920.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, murmurhash, langcodes, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\n",
            "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-embeddings-openai spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "y9SVm76h58de"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    VectorStoreIndex,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.core.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "import openai\n",
        "import time\n",
        "openai.api_key = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO21UssT6L8N"
      },
      "source": [
        "## **Load Data**\n",
        "\n",
        "Let’s load our document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "x6QdEBd-17OC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14\n"
          ]
        }
      ],
      "source": [
        "# Load Data\n",
        "# reader = SimpleDirectoryReader(\"../data/web-software-development-1-0/\", recursive=True)\n",
        "reader = SimpleDirectoryReader(\"../data/web-software-development-1-0/13-working-with-databases-i/\", recursive=True)\n",
        "\n",
        "documents = reader.load_data()\n",
        "print(len(documents))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnpPtiz56TYA"
      },
      "source": [
        "## **Question Generation**\n",
        "\n",
        "To select the right `chunk_size`, we'll compute metrics like Average Response time, Faithfulness, and Relevancy for various `chunk_sizes`. The `DatasetGenerator` will help us generate questions from the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "26BgDF3L6Z0r"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of documents:  14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/addo/.local/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:212: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
            "  return cls(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EVAL QUESTIONS ===\n",
            "['What is the importance of using a database in web applications?', 'What database management system will be used in this course?', 'What are the learning objectives related to working with databases?', 'Where can you find a tutorial for SQL basics if you need a refresher?', 'How can you start using PostgreSQL according to the document?', 'What is the recommended approach for taking PostgreSQL into use for development?', 'What is the purpose of the Walking skeleton in relation to PostgreSQL?', 'What are some options for running PostgreSQL locally?', 'Name two hosted services that provide PostgreSQL as a service.', 'Why does the document strongly recommend using the first option for development when starting to use PostgreSQL?', 'What are two options for starting to use PostgreSQL as mentioned in the document?', 'What are some examples of hosted services that provide PostgreSQL databases?', 'Why does the document strongly recommend using the first option for development?', 'How can you get started with ElephantSQL according to the document?', \"What attributes are included in the table created in the document's example using SQL?\", 'How can you add names to the table in ElephantSQL according to the document?', \"What SQL query can you use to select all rows from the 'names' table in ElephantSQL?\", \"What library is used in the document's example to access the database programatically?\", 'What information is grayed out in the image of the ElephantSQL details page?', \"What is the purpose of the 'id' attribute in the table created in the document's example?\", 'What library is used in the first example to access a PostgreSQL database in the provided code snippet?', 'How can you specify the database credentials when using Postgres.js in the provided code snippet?', 'What is the purpose of the `max: 2` parameter in the Postgres.js example?', 'In the second example, what library is used to access a PostgreSQL database?', 'What is the recommended alternative to Deno Postgres mentioned in the document?', 'How can you establish a connection to a PostgreSQL database using Deno Postgres in the provided code snippet?', 'What query is executed in the Deno Postgres example to retrieve data from the database?', 'What is the significance of having a database client when working with databases?', 'What is the default database client mentioned in the document for accessing a PostgreSQL database?', 'Where can you find a list of PostgreSQL clients for different operating systems according to the document?', 'What database driver is used when working with Deno and PostgreSQL in the provided document?', 'How can you create a database client using the Postgres.js driver?', 'In the example code provided, what SQL query is being executed to retrieve data from the database?', 'How does Postgres.js ensure safe query generation when constructing SQL queries?', 'What is the purpose of the `sql` function in the Postgres.js driver?', 'In the example code, how is the result data iterated over to print only the name property?', 'What SQL statement is used to insert data into a database in the provided document?', 'After inserting a new name into the database, how many names are present in the database according to the output?', 'What flag is required to be used with Deno when working with the Postgres.js driver?', 'How can you access the Postgres.js documentation for further details on tagged template literals?']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/addo/.local/lib/python3.11/site-packages/llama_index/core/evaluation/dataset_generation.py:309: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
            "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
          ]
        }
      ],
      "source": [
        "# To evaluate for each chunk size, we will first generate a set of 40 questions from first 20 pages.\n",
        "eval_documents = documents[:20]\n",
        "print(\"Amount of documents: \", len(eval_documents))\n",
        "\n",
        "# TODO do with more chapters, and try to instruct to create 1 question per chunk (or similar)\n",
        "\n",
        "data_generator = DatasetGenerator.from_documents(documents)\n",
        "eval_questions = data_generator.generate_questions_from_nodes(num = 40)\n",
        "print(\"=== EVAL QUESTIONS ===\")\n",
        "print(eval_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WwA-0N6dMO"
      },
      "source": [
        "## Setting Up Evaluators\n",
        "\n",
        "We are setting up the GPT-4 model to serve as the backbone for evaluating the responses generated during the experiment. Two evaluators, `FaithfulnessEvaluator` and `RelevancyEvaluator`, are initialised with the `service_context` .\n",
        "\n",
        "1. **Faithfulness Evaluator** - It is useful for measuring if the response was hallucinated and measures if the response from a query engine matches any source nodes.\n",
        "2. **Relevancy Evaluator** - It is useful for measuring if the query was actually answered by the response and measures if the response + source nodes match the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G2LoMRtr6fnG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_110624/2600024486.py:5: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n"
          ]
        }
      ],
      "source": [
        "# We will use GPT-4 for evaluating the responses\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "\n",
        "# Define service context for GPT-4 for evaluation\n",
        "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
        "\n",
        "# Define Faithfulness and Relevancy Evaluators which are based on GPT-4\n",
        "faithfulness_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)\n",
        "relevancy_gpt4 = RelevancyEvaluator(service_context=service_context_gpt4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Storing embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "\n",
        "from llama_index.core.schema import IndexNode\n",
        "from llama_index.core import (\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import SummaryIndex\n",
        "from llama_index.core.retrievers import RecursiveRetriever\n",
        "import os\n",
        "# from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "\n",
        "\n",
        "def build_index(docs, chunk_size, out_path: str):\n",
        "    print(\"Chunk size: \", chunk_size)\n",
        "\n",
        "    embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\",\n",
        "                                  chunk_size=chunk_size,\n",
        "                                  )\n",
        "    Settings.embed_model = embed_model\n",
        "\n",
        "    nodes = []\n",
        "\n",
        "    splitter = SentenceSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_size/4)\n",
        "    for idx, doc in enumerate(docs):\n",
        "        print('Splitting: ' + str(idx))\n",
        "\n",
        "        cur_nodes = splitter.get_nodes_from_documents([doc])\n",
        "        for cur_node in cur_nodes:\n",
        "            # ID will be base + parent\n",
        "            file_path = doc.metadata[\"file_path\"]\n",
        "            new_node = IndexNode(\n",
        "                text=cur_node.text or \"None\",\n",
        "                index_id=str(file_path),\n",
        "                metadata=doc.metadata,\n",
        "                # obj=doc\n",
        "            )\n",
        "            nodes.append(new_node)\n",
        "    print(\"num nodes: \" + str(len(nodes)))\n",
        "\n",
        "    # save index to disk\n",
        "    if not os.path.exists(out_path):\n",
        "        index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
        "        index.set_index_id(\"simple_index\")\n",
        "        index.storage_context.persist(f\"./{out_path}\")\n",
        "    else:\n",
        "        # rebuild storage context\n",
        "        storage_context = StorageContext.from_defaults(\n",
        "            persist_dir=f\"./{out_path}\"\n",
        "        )\n",
        "        # load index\n",
        "        index = load_index_from_storage(\n",
        "            storage_context, index_id=\"simple_index\", embed_model=embed_model\n",
        "        )\n",
        "\n",
        "    return index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunk_sizes = [128, 256, 512, 1024, 2048]\n",
        "similarities_top_k = [8, 6, 4, 2, 1]\n",
        "\n",
        "# for chunk_size in chunk_sizes:\n",
        "#     llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "#     service_context = ServiceContext.from_defaults(llm=llm, chunk_size=chunk_size)\n",
        "#     vector_index = VectorStoreIndex.from_documents(\n",
        "#         eval_documents, service_context=service_context\n",
        "#     )\n",
        "    \n",
        "#     vector_index.storage_context.persist(persist_dir=f\"vector_stores/openai_{chunk_size}-wsd\")\n",
        "#     vector_indices.append(vector_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUncIIxR6gVz"
      },
      "source": [
        "## **Response Evaluation For A Chunk Size**\n",
        "\n",
        "We evaluate each chunk_size based on 3 metrics.\n",
        "\n",
        "1. Average Response Time.\n",
        "2. Average Faithfulness.\n",
        "3. Average Relevancy.\n",
        "\n",
        "Here's a function, `evaluate_response_time_and_accuracy`, that does just that which has:\n",
        "\n",
        "1. VectorIndex Creation.\n",
        "2. Building the Query Engine**.**\n",
        "3. Metrics Calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dEC2Lr0z6p1N"
      },
      "outputs": [],
      "source": [
        "# Define function to calculate average response time, average faithfulness and average relevancy metrics for given chunk size\n",
        "# We use GPT-3.5-Turbo to generate response and GPT-4 to evaluate it.\n",
        "def evaluate_response_time_and_accuracy(chunk_size, similarity_top_k, eval_questions=eval_questions):\n",
        "    \"\"\"\n",
        "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
        "\n",
        "    Parameters:\n",
        "    chunk_size (int): The size of data chunks being processed.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    total_response_time = 0\n",
        "    total_faithfulness = 0\n",
        "    total_relevancy = 0\n",
        "\n",
        "    # Load vector_index\n",
        "    vector_index = build_index(eval_documents, chunk_size, f\"vector_stores/openai_{chunk_size}-wsd\")\n",
        "\n",
        "    # build query engine\n",
        "    # By default, similarity_top_k is set to 2. To experiment with different values, pass it as an argument to as_query_engine()\n",
        "    query_engine = vector_index.as_query_engine(similarity_top_k=similarity_top_k, embed_model=Settings.embed_model)\n",
        "    num_questions = len(eval_questions)\n",
        "\n",
        "    # Iterate over each question in eval_questions to compute metrics.\n",
        "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
        "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
        "    print(\"=========== QA pairs\")\n",
        "    for question in eval_questions:\n",
        "        print(\"--Q\\n\", question, \"\\n--\")\n",
        "        start_time = time.time()\n",
        "        response_vector = query_engine.query(question)\n",
        "        # print(\"--A\\n\", response_vector, \"\\n--\")\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
        "            query=question, response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        total_response_time += elapsed_time\n",
        "        total_faithfulness += faithfulness_result\n",
        "        total_relevancy += relevancy_result\n",
        "\n",
        "        # TODO: both response and retrieval evaluation\n",
        "\n",
        "    print(\"===========\")\n",
        "    average_response_time = total_response_time / num_questions\n",
        "    average_faithfulness = total_faithfulness / num_questions\n",
        "    average_relevancy = total_relevancy / num_questions\n",
        "\n",
        "    return average_response_time, average_faithfulness, average_relevancy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from llama_index.core.base.response.schema import Response\n",
        "\n",
        "def evaluate_response_time_and_accuracy_without_rag(eval_questions=eval_questions):\n",
        "    \"\"\"\n",
        "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
        "\n",
        "    Parameters:\n",
        "    chunk_size (int): The size of data chunks being processed.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    total_response_time = 0\n",
        "    total_faithfulness = 0\n",
        "    total_relevancy = 0\n",
        "\n",
        "    # By default, similarity_top_k is set to 2. To experiment with different values, pass it as an argument to as_query_engine()\n",
        "    num_questions = len(eval_questions)\n",
        "\n",
        "    # index = VectorStoreIndex(nodes=[])\n",
        "    # query_engine = index.as_query_engine(llm=OpenAI())\n",
        "\n",
        "\n",
        "    # Iterate over each question in eval_questions to compute metrics.\n",
        "    # While BatchEvalRunner can be used for faster evaluations (see: https://docs.llamaindex.ai/en/latest/examples/evaluation/batch_eval.html),\n",
        "    # we're using a loop here to specifically measure response time for different chunk sizes.\n",
        "    print(\"=========== QA pairs\")\n",
        "    for question in eval_questions:\n",
        "        print(\"--Q\\n\", question, \"\\n--\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        response = str(OpenAI().complete(question))\n",
        "        response_vector = Response(response)\n",
        "        # response_vector = query_engine.query(question)\n",
        "\n",
        "        print(\"--A\\n\", response, \"\\n--\")\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        faithfulness_result = faithfulness_gpt4.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        relevancy_result = relevancy_gpt4.evaluate_response(\n",
        "            query=question, response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        total_response_time += elapsed_time\n",
        "        total_faithfulness += faithfulness_result\n",
        "        total_relevancy += relevancy_result\n",
        "\n",
        "        # TODO: both response and retrieval evaluation\n",
        "\n",
        "    print(\"===========\")\n",
        "    average_response_time = total_response_time / num_questions\n",
        "    average_faithfulness = total_faithfulness / num_questions\n",
        "    average_relevancy = total_relevancy / num_questions\n",
        "\n",
        "    return average_response_time, average_faithfulness, average_relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8DQvTP96s48"
      },
      "source": [
        "## **Testing Across Different Chunk Sizes**\n",
        "\n",
        "We'll evaluate a range of chunk sizes to identify which offers the most promising metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "jlKICwXH6Tib"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk size:  128\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "num nodes: 1330\n",
            "Chunk size 128 - Similarities_top_k [8, 6, 4, 2, 1] - Average Response time: 2.23s, Average Faithfulness: 1.00, Average Relevancy: 0.97\n",
            "Chunk size:  256\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "num nodes: 574\n",
            "Chunk size 256 - Similarities_top_k [8, 6, 4, 2, 1] - Average Response time: 2.10s, Average Faithfulness: 0.97, Average Relevancy: 0.93\n",
            "Chunk size:  512\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "num nodes: 267\n",
            "Chunk size 512 - Similarities_top_k [8, 6, 4, 2, 1] - Average Response time: 1.88s, Average Faithfulness: 0.97, Average Relevancy: 0.97\n",
            "Chunk size:  1024\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "num nodes: 133\n",
            "Chunk size 1024 - Similarities_top_k [8, 6, 4, 2, 1] - Average Response time: 1.80s, Average Faithfulness: 0.90, Average Relevancy: 0.88\n",
            "Chunk size:  2048\n",
            "Splitting: 0\n",
            "Splitting: 1\n",
            "Splitting: 2\n",
            "Splitting: 3\n",
            "Splitting: 4\n",
            "Splitting: 5\n",
            "Splitting: 6\n",
            "Splitting: 7\n",
            "Splitting: 8\n",
            "Splitting: 9\n",
            "Splitting: 10\n",
            "Splitting: 11\n",
            "Splitting: 12\n",
            "Splitting: 13\n",
            "num nodes: 70\n",
            "Chunk size 2048 - Similarities_top_k [8, 6, 4, 2, 1] - Average Response time: 1.72s, Average Faithfulness: 0.80, Average Relevancy: 0.85\n"
          ]
        }
      ],
      "source": [
        "# Iterate over different chunk sizes to evaluate the metrics to help fix the chunk size.\n",
        "for chunk_size, similarity_top_k in zip(chunk_sizes, similarities_top_k):\n",
        "    avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, similarity_top_k, eval_questions=eval_questions)\n",
        "    print(f\"Chunk size {chunk_size} - Similarities_top_k {similarity_top_k} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating responses without RAG\n",
        "\n",
        "## Warning\n",
        "From the way that the questions are stated, it might be confusing for the LLM to provide a response to them or impossible to response.\n",
        "For example:\n",
        "- \"What database management system will be used in this course?\"\n",
        "- \"Why does the document strongly recommend using the first option for development when starting to use PostgreSQL?\"\n",
        " \n",
        "These have to be cleaned manually before running the evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========== QA pairs\n",
            "--Q\n",
            " What is the importance of using a database in web applications? \n",
            "--\n",
            "--A\n",
            " Using a database in web applications is important for several reasons:\n",
            "\n",
            "1. Data storage: Databases provide a structured way to store and organize data, making it easier to retrieve and manipulate information. This is essential for web applications that need to store user information, product details, and other data.\n",
            "\n",
            "2. Data retrieval: Databases allow for efficient retrieval of data, enabling web applications to quickly access and display information to users. This helps improve the performance and responsiveness of the application.\n",
            "\n",
            "3. Data consistency: Databases help maintain data consistency by enforcing rules and constraints on the data stored within them. This ensures that the data remains accurate and reliable, even as the application grows and evolves.\n",
            "\n",
            "4. Data security: Databases provide security features such as user authentication, access control, and encryption to protect sensitive information from unauthorized access. This helps ensure the confidentiality and integrity of the data stored in the database.\n",
            "\n",
            "5. Scalability: Databases can scale to accommodate large amounts of data and high volumes of traffic, making them suitable for web applications that need to handle a large number of users and transactions.\n",
            "\n",
            "Overall, using a database in web applications helps improve data management, performance, security, and scalability, making it an essential component of modern web development. \n",
            "--\n",
            "--Q\n",
            " What database management system will be used in this course? \n",
            "--\n",
            "--A\n",
            " The database management system used in this course will depend on the specific requirements and preferences of the instructor or institution offering the course. Some common database management systems used in educational settings include MySQL, Microsoft SQL Server, Oracle Database, and PostgreSQL. \n",
            "--\n",
            "--Q\n",
            " What are the learning objectives related to working with databases? \n",
            "--\n",
            "--A\n",
            " 1. Understand the basic concepts and principles of databases, including data modeling, normalization, and database design.\n",
            "2. Learn how to create, manage, and manipulate databases using a database management system (DBMS) such as MySQL, Oracle, or Microsoft SQL Server.\n",
            "3. Develop the skills to write and execute SQL queries to retrieve, update, and delete data from a database.\n",
            "4. Gain an understanding of database security principles and best practices to protect sensitive data.\n",
            "5. Learn how to optimize database performance through indexing, query optimization, and other techniques.\n",
            "6. Understand the importance of data integrity and how to enforce constraints to maintain data quality.\n",
            "7. Develop the ability to design and implement relational database schemas that meet the requirements of a given application.\n",
            "8. Learn how to work with different types of databases, including relational, NoSQL, and cloud-based databases.\n",
            "9. Gain experience with database administration tasks such as backup and recovery, monitoring, and troubleshooting.\n",
            "10. Understand how to integrate databases with other systems and applications to support business processes and decision-making. \n",
            "--\n",
            "--Q\n",
            " Where can you find a tutorial for SQL basics if you need a refresher? \n",
            "--\n",
            "--A\n",
            " There are many online resources where you can find tutorials for SQL basics. Some popular options include:\n",
            "\n",
            "1. W3Schools: A popular website that offers tutorials on various programming languages, including SQL. You can find their SQL tutorial here: https://www.w3schools.com/sql/\n",
            "\n",
            "2. SQLZoo: An interactive SQL tutorial website that allows you to practice SQL queries in a live database environment. You can access their tutorials here: https://sqlzoo.net/\n",
            "\n",
            "3. Codecademy: An online learning platform that offers interactive SQL courses for beginners. You can find their SQL course here: https://www.codecademy.com/learn/learn-sql\n",
            "\n",
            "4. Khan Academy: A non-profit educational organization that offers free online courses, including a SQL tutorial. You can access their SQL course here: https://www.khanacademy.org/computing/computer-programming/sql\n",
            "\n",
            "These are just a few options, but there are many other resources available online for learning SQL basics. Just search for \"SQL tutorial\" or \"SQL basics tutorial\" in your preferred search engine to find more options. \n",
            "--\n",
            "--Q\n",
            " How can you start using PostgreSQL according to the document? \n",
            "--\n",
            "--A\n",
            " To start using PostgreSQL according to the documentation, you can follow these steps:\n",
            "\n",
            "1. Download and install PostgreSQL: Visit the official PostgreSQL website and download the appropriate version for your operating system. Follow the installation instructions provided on the website.\n",
            "\n",
            "2. Initialize the database cluster: After installing PostgreSQL, you need to initialize the database cluster. This can be done by running the command `initdb` in the terminal or command prompt.\n",
            "\n",
            "3. Start the PostgreSQL server: Once the database cluster is initialized, you can start the PostgreSQL server by running the command `pg_ctl start` in the terminal or command prompt.\n",
            "\n",
            "4. Connect to the PostgreSQL server: You can connect to the PostgreSQL server using the `psql` command-line tool. Simply run `psql` in the terminal or command prompt to open a connection to the server.\n",
            "\n",
            "5. Create a new database: Once connected to the PostgreSQL server, you can create a new database using the `CREATE DATABASE` command. For example, `CREATE DATABASE mydatabase;` will create a new database named `mydatabase`.\n",
            "\n",
            "6. Create tables and insert data: After creating a database, you can create tables and insert data into them using SQL commands. Refer to the PostgreSQL documentation for more information on how to create tables and manipulate data.\n",
            "\n",
            "By following these steps, you can start using PostgreSQL and begin working with databases in a PostgreSQL environment. \n",
            "--\n",
            "--Q\n",
            " What is the recommended approach for taking PostgreSQL into use for development? \n",
            "--\n",
            "--A\n",
            " The recommended approach for taking PostgreSQL into use for development is as follows:\n",
            "\n",
            "1. Install PostgreSQL: Download and install the latest version of PostgreSQL from the official website or using a package manager.\n",
            "\n",
            "2. Set up a database: Create a new database using the `createdb` command or through a graphical user interface like pgAdmin.\n",
            "\n",
            "3. Connect to the database: Use a database client like psql or pgAdmin to connect to the database and start running queries.\n",
            "\n",
            "4. Create tables and insert data: Create tables and insert sample data to start developing and testing your application.\n",
            "\n",
            "5. Use SQL queries: Write SQL queries to retrieve, update, and delete data from the database.\n",
            "\n",
            "6. Use PostgreSQL features: Take advantage of advanced features like indexes, constraints, and triggers to optimize your database performance.\n",
            "\n",
            "7. Backup and restore: Regularly backup your database to prevent data loss and restore it in case of any issues.\n",
            "\n",
            "8. Use version control: Store your database schema and data in version control to track changes and collaborate with other developers.\n",
            "\n",
            "9. Test and optimize: Test your application with different data sets and optimize your queries for better performance.\n",
            "\n",
            "10. Learn and improve: Continuously learn about PostgreSQL best practices and new features to improve your development process. \n",
            "--\n",
            "--Q\n",
            " What is the purpose of the Walking skeleton in relation to PostgreSQL? \n",
            "--\n",
            "--A\n",
            " The purpose of a Walking Skeleton in relation to PostgreSQL is to set up a basic, functioning system that includes all the necessary components to support the PostgreSQL database. This includes setting up the database server, creating tables and schemas, and establishing connections to the database. The Walking Skeleton serves as a foundation for building and testing the application, allowing developers to quickly iterate and add new features on top of the existing infrastructure. It helps ensure that the PostgreSQL database is properly integrated into the overall system architecture and can support the application's functionality. \n",
            "--\n",
            "--Q\n",
            " What are some options for running PostgreSQL locally? \n",
            "--\n",
            "--A\n",
            " 1. Download and install PostgreSQL directly from the official website: You can download the installer for your operating system from the PostgreSQL website and follow the installation instructions to set up a local PostgreSQL server on your machine.\n",
            "\n",
            "2. Use a package manager: Many package managers, such as Homebrew for macOS or apt-get for Linux, offer PostgreSQL as a package that can be easily installed on your system.\n",
            "\n",
            "3. Use a PostgreSQL Docker container: You can run PostgreSQL in a Docker container on your local machine. This allows you to easily set up and manage a PostgreSQL instance without having to install it directly on your system.\n",
            "\n",
            "4. Use a PostgreSQL GUI tool: There are several GUI tools available, such as pgAdmin or DBeaver, that allow you to easily manage and interact with a local PostgreSQL database.\n",
            "\n",
            "5. Use a cloud-based PostgreSQL service: If you prefer not to run PostgreSQL locally, you can use a cloud-based PostgreSQL service such as Amazon RDS, Google Cloud SQL, or Heroku Postgres to set up and manage a PostgreSQL database in the cloud. \n",
            "--\n",
            "--Q\n",
            " Name two hosted services that provide PostgreSQL as a service. \n",
            "--\n",
            "--A\n",
            " 1. Amazon RDS (Relational Database Service)\n",
            "2. Google Cloud SQL \n",
            "--\n",
            "--Q\n",
            " Why does the document strongly recommend using the first option for development when starting to use PostgreSQL? \n",
            "--\n",
            "--A\n",
            " The document likely recommends using the first option for development when starting to use PostgreSQL because it is the simplest and most straightforward approach. This option likely involves using a local installation of PostgreSQL on the developer's machine, which allows for easy setup and configuration without the need for additional infrastructure or dependencies.\n",
            "\n",
            "Additionally, using a local installation for development purposes can help streamline the development process and make it easier for developers to quickly iterate on their code and test new features. It also allows developers to have more control over their development environment and make changes as needed without impacting other team members or production systems.\n",
            "\n",
            "Overall, using the first option for development when starting to use PostgreSQL is likely recommended because it provides a convenient and efficient way for developers to get up and running with the database and start building their applications. \n",
            "--\n",
            "--Q\n",
            " What are two options for starting to use PostgreSQL as mentioned in the document? \n",
            "--\n",
            "--A\n",
            " 1. Download and install PostgreSQL directly from the official website: Users can download the PostgreSQL installer from the official website and follow the installation instructions to set up the database on their local machine.\n",
            "\n",
            "2. Use a managed PostgreSQL service: Users can also opt to use a managed PostgreSQL service provided by cloud platforms such as Amazon Web Services (AWS), Google Cloud Platform (GCP), or Microsoft Azure. These services offer easy setup and management of PostgreSQL databases without the need to install and configure the database manually. \n",
            "--\n",
            "--Q\n",
            " What are some examples of hosted services that provide PostgreSQL databases? \n",
            "--\n",
            "--A\n",
            " 1. Amazon RDS (Relational Database Service)\n",
            "2. Google Cloud SQL\n",
            "3. Microsoft Azure Database for PostgreSQL\n",
            "4. Heroku Postgres\n",
            "5. DigitalOcean Managed Databases\n",
            "6. ScaleGrid\n",
            "7. Aiven\n",
            "8. EnterpriseDB Cloud Database Service\n",
            "9. ElephantSQL\n",
            "10. ObjectRocket by Rackspace \n",
            "--\n",
            "--Q\n",
            " Why does the document strongly recommend using the first option for development? \n",
            "--\n",
            "--A\n",
            " The document may strongly recommend using the first option for development because it is considered to be the most efficient, cost-effective, or reliable option compared to the other options. It may have been thoroughly researched and tested, and found to be the best choice based on various factors such as performance, scalability, ease of implementation, and compatibility with existing systems. Additionally, the first option may align more closely with the project requirements and goals, making it the most suitable choice for development. \n",
            "--\n",
            "--Q\n",
            " How can you get started with ElephantSQL according to the document? \n",
            "--\n",
            "--A\n",
            " To get started with ElephantSQL, you can follow these steps outlined in the documentation:\n",
            "\n",
            "1. Sign up for an account on the ElephantSQL website.\n",
            "2. Once you have created an account, you can create a new database instance by clicking on the \"Create New\" button.\n",
            "3. Choose a plan that fits your needs and select the region where you want your database to be hosted.\n",
            "4. Configure your database instance by setting a name, username, and password for your database.\n",
            "5. Once your database instance is created, you can connect to it using the provided connection string and start using it in your applications.\n",
            "\n",
            "Additionally, the documentation provides detailed instructions on how to connect to your database using various programming languages and tools, as well as tips on how to optimize your database performance and security. \n",
            "--\n",
            "--Q\n",
            " What attributes are included in the table created in the document's example using SQL? \n",
            "--\n",
            "--A\n",
            " The attributes included in the table created in the document's example using SQL are:\n",
            "\n",
            "1. ID (Primary Key)\n",
            "2. Name\n",
            "3. Age\n",
            "4. Gender\n",
            "5. Occupation \n",
            "--\n",
            "--Q\n",
            " How can you add names to the table in ElephantSQL according to the document? \n",
            "--\n",
            "--A\n",
            " To add names to a table in ElephantSQL, you can use SQL commands to insert data into the table. Here is an example of how you can add names to a table named \"users\":\n",
            "\n",
            "1. Connect to your ElephantSQL database using a SQL client or command line interface.\n",
            "2. Use the following SQL command to insert a new row with a name into the \"users\" table:\n",
            "\n",
            "```sql\n",
            "INSERT INTO users (name) VALUES ('John Doe');\n",
            "```\n",
            "\n",
            "3. You can repeat this process for each name you want to add to the table by changing the value in the `VALUES` clause.\n",
            "\n",
            "4. You can also insert multiple rows at once by separating each set of values with a comma, like this:\n",
            "\n",
            "```sql\n",
            "INSERT INTO users (name) VALUES ('Jane Smith'), ('Alice Johnson'), ('Bob Brown');\n",
            "```\n",
            "\n",
            "5. After executing the SQL command, you should see the new names added to the \"users\" table in your ElephantSQL database. \n",
            "--\n",
            "--Q\n",
            " What SQL query can you use to select all rows from the 'names' table in ElephantSQL? \n",
            "--\n",
            "--A\n",
            " To select all rows from the 'names' table in ElephantSQL, you can use the following SQL query:\n",
            "\n",
            "```sql\n",
            "SELECT * FROM names;\n",
            "``` \n",
            "--\n",
            "--Q\n",
            " What library is used in the document's example to access the database programatically? \n",
            "--\n",
            "--A\n",
            " The document's example uses the \"sqlite3\" library to access the database programmatically. \n",
            "--\n",
            "--Q\n",
            " What information is grayed out in the image of the ElephantSQL details page? \n",
            "--\n",
            "--A\n",
            " The information that is grayed out in the image of the ElephantSQL details page is the \"Plan\" and \"Region\" sections. \n",
            "--\n",
            "--Q\n",
            " What is the purpose of the 'id' attribute in the table created in the document's example? \n",
            "--\n",
            "--A\n",
            " The 'id' attribute in the table created in the document's example is used to uniquely identify each row in the table. This attribute is commonly used in web development to target specific elements for styling or manipulation using CSS or JavaScript. It allows developers to easily reference and interact with individual rows in the table. \n",
            "--\n",
            "--Q\n",
            " What library is used in the first example to access a PostgreSQL database in the provided code snippet? \n",
            "--\n",
            "--A\n",
            " The library used in the first example to access a PostgreSQL database is \"pg-promise\". \n",
            "--\n",
            "--Q\n",
            " How can you specify the database credentials when using Postgres.js in the provided code snippet? \n",
            "--\n",
            "--A\n",
            " To specify the database credentials when using Postgres.js, you can pass an object containing the connection details as an argument when creating a new instance of the `Client` class. Here is an example code snippet showing how to specify the database credentials:\n",
            "\n",
            "```javascript\n",
            "const { Client } = require('pg');\n",
            "\n",
            "const client = new Client({\n",
            "  user: 'your_username',\n",
            "  host: 'your_host',\n",
            "  database: 'your_database',\n",
            "  password: 'your_password',\n",
            "  port: 'your_port',\n",
            "});\n",
            "\n",
            "client.connect();\n",
            "\n",
            "// Your database queries here\n",
            "\n",
            "client.end();\n",
            "```\n",
            "\n",
            "In the above code snippet, you need to replace `'your_username'`, `'your_host'`, `'your_database'`, `'your_password'`, and `'your_port'` with your actual database credentials. Once you have specified the credentials, you can use the `client` object to execute your database queries. \n",
            "--\n",
            "--Q\n",
            " What is the purpose of the `max: 2` parameter in the Postgres.js example? \n",
            "--\n",
            "--A\n",
            " The `max: 2` parameter in the Postgres.js example specifies the maximum number of connections that can be opened to the PostgreSQL database at any given time. This parameter helps to limit the number of concurrent connections to the database, which can prevent overwhelming the database server with too many requests and potentially causing performance issues. By setting a maximum limit on the number of connections, it helps to manage the resources effectively and ensure that the database can handle the incoming requests efficiently. \n",
            "--\n",
            "--Q\n",
            " In the second example, what library is used to access a PostgreSQL database? \n",
            "--\n",
            "--A\n",
            " The library used to access a PostgreSQL database in the second example is psycopg2. \n",
            "--\n",
            "--Q\n",
            " What is the recommended alternative to Deno Postgres mentioned in the document? \n",
            "--\n",
            "--A\n",
            " The recommended alternative to Deno Postgres mentioned in the document is `deno_mysql`. \n",
            "--\n",
            "--Q\n",
            " How can you establish a connection to a PostgreSQL database using Deno Postgres in the provided code snippet? \n",
            "--\n",
            "--A\n",
            " To establish a connection to a PostgreSQL database using Deno Postgres, you can use the following code snippet:\n",
            "\n",
            "```typescript\n",
            "import { Client } from \"https://deno.land/x/postgres/mod.ts\";\n",
            "\n",
            "const client = new Client({\n",
            "  user: \"your_username\",\n",
            "  database: \"your_database\",\n",
            "  hostname: \"localhost\",\n",
            "  port: 5432,\n",
            "  password: \"your_password\",\n",
            "});\n",
            "\n",
            "await client.connect();\n",
            "\n",
            "// Now you can execute queries on the connected database\n",
            "\n",
            "// Example query\n",
            "const result = await client.query(\"SELECT * FROM your_table\");\n",
            "\n",
            "console.log(result.rows);\n",
            "\n",
            "await client.end();\n",
            "```\n",
            "\n",
            "Make sure to replace `your_username`, `your_database`, `localhost`, `5432`, and `your_password` with your actual PostgreSQL database credentials. This code snippet creates a new `Client` instance with the provided configuration and connects to the PostgreSQL database. You can then execute queries on the connected database using the `client.query()` method. Finally, don't forget to close the connection using `client.end()` when you are done with the database operations. \n",
            "--\n",
            "--Q\n",
            " What query is executed in the Deno Postgres example to retrieve data from the database? \n",
            "--\n",
            "--A\n",
            " In the Deno Postgres example, the query executed to retrieve data from the database is:\n",
            "\n",
            "```typescript\n",
            "const result = await client.queryObject(\"SELECT * FROM users\");\n",
            "``` \n",
            "\n",
            "This query selects all columns from the \"users\" table in the database. \n",
            "--\n",
            "--Q\n",
            " What is the significance of having a database client when working with databases? \n",
            "--\n",
            "--A\n",
            " A database client is a software application that allows users to interact with a database management system (DBMS) to perform various tasks such as querying, updating, and managing data. The significance of having a database client when working with databases includes:\n",
            "\n",
            "1. Ease of use: Database clients provide a user-friendly interface that allows users to interact with the database without needing to write complex SQL queries or commands. This makes it easier for users to access and manipulate data in the database.\n",
            "\n",
            "2. Improved productivity: Database clients often have features such as query builders, data visualization tools, and automation capabilities that help users perform tasks more efficiently. This can help improve productivity and save time when working with databases.\n",
            "\n",
            "3. Security: Database clients provide secure access to the database by allowing users to authenticate and control access permissions. This helps protect sensitive data and prevent unauthorized access to the database.\n",
            "\n",
            "4. Performance monitoring: Database clients often include tools for monitoring database performance, such as query execution times, resource usage, and bottlenecks. This can help users optimize database performance and troubleshoot issues more effectively.\n",
            "\n",
            "5. Collaboration: Database clients often support features for sharing queries, results, and reports with other users. This can facilitate collaboration and communication among team members working on the same database.\n",
            "\n",
            "Overall, having a database client when working with databases can help users interact with the database more effectively, improve productivity, enhance security, monitor performance, and facilitate collaboration. \n",
            "--\n",
            "--Q\n",
            " What is the default database client mentioned in the document for accessing a PostgreSQL database? \n",
            "--\n",
            "--A\n",
            " The default database client mentioned in the document for accessing a PostgreSQL database is psql. \n",
            "--\n",
            "--Q\n",
            " Where can you find a list of PostgreSQL clients for different operating systems according to the document? \n",
            "--\n",
            "--A\n",
            " You can find a list of PostgreSQL clients for different operating systems in the official PostgreSQL documentation under the section \"Client Interfaces\". \n",
            "--\n",
            "--Q\n",
            " What database driver is used when working with Deno and PostgreSQL in the provided document? \n",
            "--\n",
            "--A\n",
            " The document mentions that the `deno-postgres` library is used as the database driver when working with Deno and PostgreSQL. \n",
            "--\n",
            "--Q\n",
            " How can you create a database client using the Postgres.js driver? \n",
            "--\n",
            "--A\n",
            " To create a database client using the Postgres.js driver, you can follow these steps:\n",
            "\n",
            "1. Install the Postgres.js driver by running the following command in your terminal:\n",
            "```\n",
            "npm install pg\n",
            "```\n",
            "\n",
            "2. Create a new JavaScript file (e.g., `databaseClient.js`) and require the `pg` module at the top of the file:\n",
            "```javascript\n",
            "const { Client } = require('pg');\n",
            "```\n",
            "\n",
            "3. Create a new instance of the `Client` class and configure it with your database connection details:\n",
            "```javascript\n",
            "const client = new Client({\n",
            "  user: 'your_username',\n",
            "  host: 'localhost',\n",
            "  database: 'your_database_name',\n",
            "  password: 'your_password',\n",
            "  port: 5432,\n",
            "});\n",
            "```\n",
            "\n",
            "4. Connect to the database using the `connect` method of the client instance:\n",
            "```javascript\n",
            "client.connect();\n",
            "```\n",
            "\n",
            "5. Execute SQL queries using the `query` method of the client instance. For example, you can execute a simple SELECT query like this:\n",
            "```javascript\n",
            "client.query('SELECT * FROM your_table', (err, res) => {\n",
            "  if (err) {\n",
            "    console.error(err);\n",
            "    return;\n",
            "  }\n",
            "  console.log(res.rows);\n",
            "});\n",
            "```\n",
            "\n",
            "6. Finally, don't forget to close the database connection when you're done using the `end` method of the client instance:\n",
            "```javascript\n",
            "client.end();\n",
            "```\n",
            "\n",
            "By following these steps, you can create a database client using the Postgres.js driver and interact with your PostgreSQL database in your Node.js application. \n",
            "--\n",
            "--Q\n",
            " In the example code provided, what SQL query is being executed to retrieve data from the database? \n",
            "--\n",
            "--A\n",
            " SELECT * FROM users WHERE age > 18; \n",
            "--\n",
            "--Q\n",
            " How does Postgres.js ensure safe query generation when constructing SQL queries? \n",
            "--\n",
            "--A\n",
            " Postgres.js ensures safe query generation by using parameterized queries. Parameterized queries separate the SQL query logic from the user input, preventing SQL injection attacks. User input is treated as data values rather than part of the SQL query itself, which helps to prevent malicious code from being injected into the query.\n",
            "\n",
            "Additionally, Postgres.js may also provide input validation and sanitization functions to further ensure the safety of the generated SQL queries. These functions can help to filter out potentially harmful input before it is included in the query.\n",
            "\n",
            "Overall, by using parameterized queries and implementing input validation and sanitization, Postgres.js helps to ensure safe query generation and protect against SQL injection attacks. \n",
            "--\n",
            "--Q\n",
            " What is the purpose of the `sql` function in the Postgres.js driver? \n",
            "--\n",
            "--A\n",
            " The `sql` function in the Postgres.js driver is used to create SQL query strings in a safe and secure way. It helps prevent SQL injection attacks by automatically escaping and sanitizing user input before constructing the query. This function allows developers to easily build dynamic SQL queries without having to manually handle escaping and sanitizing input values. \n",
            "--\n",
            "--Q\n",
            " In the example code, how is the result data iterated over to print only the name property? \n",
            "--\n",
            "--A\n",
            " The result data is iterated over using a for loop. Within the loop, the name property of each object in the result data is accessed and printed to the console.\n",
            "\n",
            "Here is an example code snippet:\n",
            "\n",
            "```javascript\n",
            "const resultData = [\n",
            "  { name: 'Alice', age: 25 },\n",
            "  { name: 'Bob', age: 30 },\n",
            "  { name: 'Charlie', age: 35 }\n",
            "];\n",
            "\n",
            "for (let i = 0; i < resultData.length; i++) {\n",
            "  console.log(resultData[i].name);\n",
            "}\n",
            "```\n",
            "\n",
            "In this code snippet, the for loop iterates over each object in the result data array. For each object, the name property is accessed using `resultData[i].name` and printed to the console using `console.log()`. This will output only the name property of each object in the result data array. \n",
            "--\n",
            "--Q\n",
            " What SQL statement is used to insert data into a database in the provided document? \n",
            "--\n",
            "--A\n",
            " The SQL statement used to insert data into a database in the provided document is:\n",
            "\n",
            "```sql\n",
            "INSERT INTO table_name (column1, column2, column3, ...)\n",
            "VALUES (value1, value2, value3, ...);\n",
            "``` \n",
            "--\n",
            "--Q\n",
            " After inserting a new name into the database, how many names are present in the database according to the output? \n",
            "--\n",
            "--A\n",
            " Without knowing the specific output, it is not possible to determine the exact number of names present in the database after inserting a new name. The number of names present in the database would depend on the initial number of names in the database and how many names were added or removed before and after inserting the new name. \n",
            "--\n",
            "--Q\n",
            " What flag is required to be used with Deno when working with the Postgres.js driver? \n",
            "--\n",
            "--A\n",
            " The --unstable flag is required to be used with Deno when working with the Postgres.js driver. This flag enables the use of unstable APIs, which are necessary for interacting with databases in Deno. \n",
            "--\n",
            "--Q\n",
            " How can you access the Postgres.js documentation for further details on tagged template literals? \n",
            "--\n",
            "--A\n",
            " You can access the Postgres.js documentation for further details on tagged template literals by visiting the official website of Postgres.js and navigating to the documentation section. Look for the section related to tagged template literals or search for it using the search bar on the website. The documentation should provide detailed information, examples, and usage guidelines for tagged template literals in Postgres.js. \n",
            "--\n",
            "===========\n",
            "Average Response time: 2.46s, Average Faithfulness: 0.00, Average Relevancy: 0.00\n"
          ]
        }
      ],
      "source": [
        "avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy_without_rag(eval_questions=eval_questions)\n",
        "print(f\"Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
